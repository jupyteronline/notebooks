{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instagram 스크랩\n",
    "\n",
    "\n",
    "## 필요한 라이브러리 설치\n",
    "* 아나콘다 사용시 다음의 프롬프트 창을 열어 conda 명령어로 설치합니다.\n",
    "* pip 사용시 아래에 있는 명령어를 터미널로 설치합니다.\n",
    "<img src=\"https://i.imgur.com/Sar4gdw.jpg\">\n",
    "\n",
    "### Selenium\n",
    "* `conda install -c anaconda selenium`\n",
    "* [Selenium :: Anaconda Cloud](https://anaconda.org/anaconda/selenium)\n",
    "\n",
    "* pip 사용시 : `pip install selenium`\n",
    "\n",
    "### BeautifulSoup\n",
    "* `conda install -c anaconda beautifulsoup4`\n",
    "* [Beautifulsoup4 :: Anaconda Cloud](https://anaconda.org/anaconda/beautifulsoup4)\n",
    "\n",
    "* pip 사용시 : `pip install beautifulsoup4`\n",
    "\n",
    "### tqdm\n",
    "* `conda install -c conda-forge tqdm`\n",
    "* [tqdm/tqdm: A Fast, Extensible Progress Bar for Python and CLI](https://github.com/tqdm/tqdm)\n",
    "* `pip install tqdm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 로드\n",
    "# requests는 작은 웹브라우저로 웹사이트 내용을 가져온다.\n",
    "import requests\n",
    "# BeautifulSoup 을 통해 읽어 온 웹페이지를 파싱한다.\n",
    "from bs4 import BeautifulSoup as bs\n",
    "# 크롤링 후 결과를 데이터프레임 형태로 보기 위해 불러온다.\n",
    "import pandas as pd\n",
    "from pandas import read_excel\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "from tqdm import trange\n",
    "from tqdm import tqdm\n",
    "\n",
    "options = webdriver.ChromeOptions()\n",
    "options.headless = True\n",
    "options.add_argument(\"window-size=1920x1080\") # 가상화면 크기\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.135 Safari/537.36\")\n",
    "\n",
    "# 브라우저 기동\n",
    "driver = webdriver.Chrome(options=options)\n",
    "#driver = webdriver.Firefox()\n",
    "\n",
    "# 인스타그램 로그인\n",
    "id = 'inhwan.jung@gmail.com'\n",
    "pw = '2020@Alpha'\n",
    "phone = '821020843388'\n",
    "\n",
    "# 클립보드에 input을 복사한 뒤\n",
    "# 해당 내용을 actionChain을 이용해 로그인 폼에 붙여넣기\n",
    "driver.get('https://www.instagram.com/accounts/login')\n",
    "# 모든 동작마다 크롬브라우저가 준비될 때 까지 최대 3초씩 대기\n",
    "driver.implicitly_wait(3)\n",
    "\n",
    "driver.find_element_by_xpath('//*[@id=\"loginForm\"]/div/div[1]/div/label/input').send_keys(id)\n",
    "time.sleep(1)\n",
    "driver.find_element_by_xpath('//*[@id=\"loginForm\"]/div/div[2]/div/label/input').send_keys(pw)\n",
    "time.sleep(1)\n",
    "driver.find_element_by_xpath('//*[@id=\"loginForm\"]/div/div[3]').click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1인 외식', '가치소비', 'RMR', '웰빙', '체험소비', '골목상권', '공유경제', '구독서비스', '로컬푸드', '모디슈머', '1인 미디어', '배달음식', '푸드홀', '스내킹 스토어', '스몰 럭셔리', '언택트', '에스닉푸드', '이색 식재료', '뉴밀리어', '지속가능', '친환경', '취향의 세분화', '콘텐츠', '콜라보레이션', '탈도심화 ', '특수부위', '펫', '푸드테크', 'K-푸드', '홈코노미', 'MZ세대', '럭셔리 미식', '듀얼매장', '무알콜', '안심식당']\n"
     ]
    }
   ],
   "source": [
    "# 핵심단어 읽어 오기\n",
    "my_sheet = '소비키워드'\n",
    "keywords_filename = 'deskresearch.xlsx'\n",
    "df = read_excel(keywords_filename, sheet_name = my_sheet, header=1) # index_col='번호'\n",
    "keywords = df['핵심단어'].values.tolist()\n",
    "print(keywords)\n",
    "\n",
    "# \"1인 외식\" 을 instagram 에서 검색 못하게 해서 제거하고 돌림\n",
    "keywords = keywords[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords\n",
    "keywords = keywords[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                     | 3/150 [00:09<07:42,  3.14s/it]"
     ]
    }
   ],
   "source": [
    "#get old tweets\n",
    "# filename_list = []\n",
    "# import csv\n",
    "\n",
    "interval = 2\n",
    "\n",
    "for keyword in keywords:\n",
    "    \n",
    "    url = f\"https://www.instagram.com/explore/tags/{keyword}\"\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # 이미지 목록을 저장할 빈 리스트\n",
    "    links = []\n",
    "    link = \"\"\n",
    "    \n",
    "    # 현재 문서 높이를 가져와서 저장\n",
    "    prev_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    # 지정된 회차 동안 반복하면서 스크롤을 화면 맨 아래로 이동한다.\n",
    "    for i in trange(0, 150):\n",
    "\n",
    "        # 수집 과정을 출력한다.\n",
    "#         print(\"%04d번째 페이지에서 %02d건 수집함 >> 누적 데이터수: %05d\" % (i+1, len(link), len(links)))\n",
    "        current_len = len(links)\n",
    "\n",
    "        # 스크롤을 가장 아래로 내림\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight)\") \n",
    "        \n",
    "        # 페이지 로딩 대기\n",
    "        time.sleep(interval)  \n",
    "\n",
    "        # 현재 브라우저에 표시되고 있는 소스코드 가져오기\n",
    "        soup = bs(driver.page_source, 'html.parser')\n",
    "        \n",
    "        # srcset이라는 속성을 포함하는 모든 이미지 태그 가져오기 --> 리스트형으로 반환됨\n",
    "        link = soup.select(\"a[href]\")\n",
    "\n",
    "        # 미리 준비한 리스트에 결합시킴\n",
    "        links += link\n",
    "\n",
    "        # 동일한 항목에 대한 중복제거\n",
    "        links = list(set(links))\n",
    "        \n",
    "        # 현재 문서 뫂이를 가져와서 저장\n",
    "        curr_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if curr_height == prev_height:\n",
    "            break\n",
    "        \n",
    "        if current_len == len(links):\n",
    "            break\n",
    "        \n",
    "        # 다음 컨텐츠가 로딩되는 동안 1초씩 대기\n",
    "        time.sleep(1)\n",
    "    \n",
    "    link_dict = { \"link\": links }\n",
    "    link_df = pd.DataFrame(link_dict)\n",
    "    # save to csv\n",
    "    filename = \"./urls/\" + \"insta-scrapped_\" + keyword.replace(\" \",\"\") + \".csv\"   \n",
    "    link_df.to_csv(filename, date_format='%Y%m%d', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
