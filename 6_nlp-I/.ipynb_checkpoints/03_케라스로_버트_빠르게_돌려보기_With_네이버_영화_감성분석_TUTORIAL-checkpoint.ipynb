{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jupyteronline/notebooks/blob/master/6_nlp/03_케라스로_버트_빠르게_돌려보기_With_네이버_영화_감성분석_TUTORIAL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fn7pKyD8dBEq"
   },
   "source": [
    "<h1><strong>케라스로 버트 빠르게 돌려보기 With 네이버 영화 평가 감성분석</strong></h1>\n",
    "<h1><strong><u>Youtube 공무원AI</u></strong></h1>\n",
    "\n",
    "![대체 텍스트](https://i.imgur.com/XSuDmcC.png)\n",
    "\n",
    "이번 튜토리얼에서는 케라스를 활용하여 자연어 처리에 아주 뛰어난 성적을 보여주고 있는 구글의 버트 모형으로 <strong>네이버 영화 평가 감성분석</strong>을 실습해보고자 합니다.\n",
    "이번 튜토리얼에서는 버트 모형을 활용하여 영화 평가가 긍정적인지 부정적인지 예측하는 인공지능을 10분안에 코딩해서 생성할 것입니다.\n",
    "케라스를 활용하였는데요, 인터넷을 찾아 보면 Torch로 쓰여진 자료는 약간 존재하는데 케라스로 쓰여진 자료는 부족하여 케라스로 한번 작성해 보았습니다.\n",
    "누구나 장소와 장비에 구애받지 않고 쉽게 따라할 수 있도록 구글 Colaboratory 기반으로 작성하였습니다.\n",
    "\n",
    "\n",
    "튜토리얼을 만들기 위해서  \n",
    "\n",
    "https://pypi.org/project/keras-bert/,   \n",
    "https://github.com/CyberZHG/keras-bert/tree/master/keras_bert 등을 참조하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_LeBONl3Rt6J"
   },
   "source": [
    "<h2><strong> 1. 버트 모형 소개 </strong></h2>\n",
    "<p> 버트 모형은 2018년 11월 구글이 공개한 인공지능(AI) 언어 모델로써 기존 자연어 모델의 성능을 가볍게 뛰어넘었으며, 일부 성능 평가에서는 인간보다 더 높은 성능을 기록한 모델입니다. <strong>BERT는 Bidirectional Encoder Representations from Transformers</strong>의 약자로 18년 10월 구글에 의해 논문이 공개되었습니다.<br>\n",
    "버트 모형의 특징은 <strong>사전학습, 문맥학습, 파인튜닝</strong>을 들 수 있습니다.<br>\n",
    "<br><strong>(사전학습)</strong> 위키피디아 같은 아주 큰 데이터들을 사용하여 '언어 이해' 모델을 사전학습(Pre-training)한다. 광범위한 데이터로 인간이 살아오면서 체득한 지식과 같이 다양한 데이터를 학습하게 됩니다.<br>\n",
    "<strong>(문맥학습1)</strong> 문장 순서를 학습하여 다음에 나온 문장이 순서에 맞는 문장인지 학습합니다.<br><br>&nbsp;&nbsp;=>  문장1: 저 남자는 회사에 출근했다<br>&nbsp;&nbsp;=>  문장2: 회사에 출근하자마자 저 남자는 커피를 끓여 마셨다.<strong>    (순서가 맞음)</strong><br><br>&nbsp;&nbsp;=> 문장3: 저 여자는 퇴근하려 한다.<br>&nbsp;&nbsp;=> 문장4: 강아지는 예쁘다.<strong>    (순서가 틀린 문장)</strong><br><br><strong>(문맥학습2)</strong> 양방향으로 학습하여 가려진 단어를 맞춘다.<br>&nbsp;&nbsp;&nbsp;&nbsp;문장 : 저 남자는 (①)에 출근했다. 회사에 출근하자마자 저 남자는 (②)를 마셨다.<br>&nbsp;&nbsp;=> ① : 회사 ② : 커피<br><br><strong>(파인튜닝)</strong> 이미 사람처럼 광범위한 글을 학습한 인공지능을 기반으로 하여 새로운 과제를 해결하게 됩니다. 이미 기반 지식이 있기 때문에 새로운 것을 쉽고 빠르게 학습할 수 있습니다.<br>\n",
    "본 실습에서는 네이버 영화 평가 댓글로 긍정 부정인지를 예측하는 버트 모형을 생성해 보겠습니다.</p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhODHE2XhI2j"
   },
   "source": [
    "<h2><strong>2. 버트라는 도구를 활용하여 네이버 영화 평가 댓글 긍정 부정 예측하기</strong></h2>\n",
    "\n",
    "<p> 버트라는 모형에 영화를 평가하는 댓글을 인풋으로 넣으면 평가가 긍정인지 부정인지를 예측하게 됩니다. 긍정에 가까우면 1을, 부정에 가까우면 0을 출력하게 됩니다.<br>\n",
    "함수로 표현하면 버트 모형 함수 <strong>function(영화댓글) = 1 or 0</strong>입니다.<br>\n",
    "그림으로 살펴보도록 하겠습니다.</p>\n",
    "\n",
    "![대체 텍스트](https://i.imgur.com/YmdAqbT.png)\n",
    "\n",
    "<p> 네이버 영화 감성분석을 위해 파일을 불러왔습니다. 긍정은 1, 부정은 0으로 라벨링 되어 있습니다. 이 데이터를 활용하여 댓글을 인풋으로 넣으면 긍정인지 부정인지 예측하는 값을 예측하는 인공지능 모형을 생성합니다.<br> 본 실습에서는 광범위한 데이터로 사전 훈련된 인공지능 모형을 네이버 영화 감성 분석 데이터로 파인튜닝(추가학습)하여 아래 그림과 같은 긍정 부정을 예측하는 모형을 만들고자 합니다.</p>\n",
    "\n",
    "![대체 텍스트](https://i.imgur.com/zNXVb3G.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwFvMz0AwGJ-"
   },
   "source": [
    "<h2><strong>3. 버트 사전학습 모형 가져오기</strong></h2>\n",
    "\n",
    "https://github.com/google-research/bert 에 접속하셔서 BERT-Base, Multilingual Cased: 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters 파일을 다운 받으시길 바랍니다.\n",
    "\n",
    "![대체 텍스트](https://i.imgur.com/RlutYyW.png)\n",
    "\n",
    "<p>사전 학습 모형을 다운 받으셨다면, Colab에서 활용하기 위해 구글 GDRIVE에 업로드 하시길 바랍니다.</p>\n",
    "\n",
    "![대체 텍스트](https://i.imgur.com/uM97zAQ.png)\n",
    "\n",
    "\n",
    "\n",
    "<p> 누구나 쉽게 실습할 수 있도록 구글 Colaboratory를 활용하였습니다<br> <u>런타임->런타임 유형 변경에서 GPU를 꼭 선택하시기 바랍니다.</u><br>\n",
    "그리고 데이터는 구글 G드라이브에 넣어 두었습니다. 사정에 맞게 폴더 경로를 변경하시기 바랍니다.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8U2uEy4pwa09"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "# wget을 활용해서 bert 모델 다운로드 가능\n",
    "import os\n",
    "!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n",
    "\n",
    "if \"bert\" not in os.listdir():\n",
    "  os.makedirs(\"bert\")\n",
    "else:\n",
    "  pass\n",
    "\n",
    "import zipfile\n",
    "import shutil\n",
    "         \n",
    "bert_zip = zipfile.ZipFile('multi_cased_L-12_H-768_A-12.zip')\n",
    "bert_zip.extractall('bert')\n",
    " \n",
    "bert_zip.close()\n",
    "\n",
    "def copytree(src, dst, symlinks=False, ignore=None):\n",
    "    for item in os.listdir(src):\n",
    "        s = os.path.join(src, item)\n",
    "        d = os.path.join(dst, item)\n",
    "        if os.path.isdir(s):\n",
    "            shutil.copytree(s, d, symlinks, ignore)\n",
    "        else:\n",
    "            shutil.copy2(s, d)\n",
    "\n",
    "copytree(\"bert/multi_cased_L-12_H-768_A-12\", \"bert\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HQ6glcvV1h73"
   },
   "source": [
    "<h2><strong>본격적으로 케라스와 버트를 활용하여 네이버 영화 댓글 감성 분석 예측을 시작하겠습니다.</strong></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cp6z2_gFzr3A"
   },
   "source": [
    "\n",
    " #### 구글 드라이브와 Colab을 연동합니다\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ybVfw9QC8KI1"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OnDgLw3ifvZI"
   },
   "outputs": [],
   "source": [
    "os.listdir('gdrive/My Drive/Colab Notebooks/naver_sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QlYre0_Pz6Zf"
   },
   "source": [
    "- Colab에서 Bert 모형을 가지고 있는 폴더를 지정해줍니다  \n",
    "\n",
    "- 파이썬 명령어 os.listdir(path)를 활용하여 폴더가 잘 있는지 확인하면서 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5x9jXyzYLtG3"
   },
   "outputs": [],
   "source": [
    "path = \"gdrive/My Drive/Colab Notebooks/naver_sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QVGCHkKN0TZ3"
   },
   "source": [
    "텐서플로우, 판다스, 넘파이, 케라스 등 필요한 모듈들을 임포트합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ffa---Ov8WfF"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np  \n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import keras as keras\n",
    "from keras.models import load_model\n",
    "from keras import backend as K\n",
    "from keras import Input, Model\n",
    "from keras import optimizers\n",
    "\n",
    "import codecs\n",
    "from tqdm import tqdm\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Wc0T-P3kuP7"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3bWhmOP0mOv"
   },
   "source": [
    "케라스에서 Bert 활용을 쉽게 만들어주는 모듈 keras-bert를 설치합니다<br>그리고 Adam optimizer의 수정판인 keras-radam 모듈을 임포트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09QnmSZXGbbD"
   },
   "outputs": [],
   "source": [
    "!pip install keras-bert\n",
    "!pip install keras-radam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cyIoeOBH04lW"
   },
   "source": [
    "keras-bert 라이브러리에서 버트 모형 활용에 필요한 모듈들을 임포트합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZv-5ALnGdwI"
   },
   "outputs": [],
   "source": [
    "from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n",
    "from keras_bert import Tokenizer\n",
    "from keras_bert import AdamWarmup, calc_train_steps\n",
    "\n",
    "from keras_radam import RAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uat6PNJbeQ7z"
   },
   "outputs": [],
   "source": [
    "os.listdir('bert')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eoez6gHVGeP8"
   },
   "source": [
    "네이버 영화 감성분석 데이터를 다운로드 합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2c8cHW7TMSi9"
   },
   "outputs": [],
   "source": [
    "# 네이버 영화 감성분석 데이터 다운로드\n",
    "!git clone https://github.com/e9t/nsmc.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ElSTNLUNeWyJ"
   },
   "outputs": [],
   "source": [
    "os.listdir('nsmc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QElcdDWGlOL"
   },
   "source": [
    "nsmc 폴더에 네이버 감성분석 데이터가 다운로드 됩니다.<br> 파일이 잘 있는지 확인해 봅니다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-VP46O2aGtG-"
   },
   "source": [
    "버트 모형을 훈련시킬 train 데이터와 test데이터를 pandas 테이블 형식으로 로드합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9j2vCEzCMW4K"
   },
   "outputs": [],
   "source": [
    "train = pd.read_table(\"nsmc/\"+\"ratings_train.txt\")\n",
    "test = pd.read_table(\"nsmc/\"+\"ratings_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PnnEcb9mQxV1"
   },
   "source": [
    "네이버 감성분석을 위한 훈련 데이터가 잘 로드되었습니다.  \n",
    "**document 칼럼에는 문장이, label 칼럼에는 긍정(1) 부정(0)이 들어갑니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "waSYuY0kMfW5"
   },
   "outputs": [],
   "source": [
    "train[50:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-NebUQmG1w-"
   },
   "source": [
    "\n",
    "- bert 훈련을 위한 사전 설정을 합니다. SEQ_LEN은 문장의 최대 길이입니다. SEQ_LEN 보다 문장의 길이가 작다면 남은 부분은 0이 채워지고, 만약에 SEQ_LEN보다 문장 길이가 길다면 SEQ_LEN을 초과하는 부분이 잘리게 됩니다.  \n",
    "- BATCH_SIZE는 메모리 초과 같은 문제를 방지하기 위해 작은 수인 16으로 정했습니다. 그리고 총 훈련 에포크 수는 2로 정했습니다. 학습율(LR;Learning rate)은 1e-5로 작게 정했습니다.  \n",
    "- pretrained_path는 bert 사전학습 모형이 있는 폴더를 의미합니다.\n",
    "- 그리고 우리가 분석할 문장이 들어있는 칼럼의 제목인 document와 긍정인지 부정인지 알려주는 칼럼을 label로 정해줍니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLYWgZJAtsku"
   },
   "outputs": [],
   "source": [
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS=2\n",
    "LR=1e-5\n",
    "\n",
    "pretrained_path =\"bert\"\n",
    "config_path = os.path.join(pretrained_path, 'bert_config.json')\n",
    "checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n",
    "vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n",
    "\n",
    "DATA_COLUMN = \"document\"\n",
    "LABEL_COLUMN = \"label\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaU-13a9H4YJ"
   },
   "source": [
    "vocab.txt에 있는 단어에 인덱스를 추가해주는 token_dict라는 딕셔너리를 생성합니다.  \n",
    "우리가 분석할 문장이 토큰화가 되고, 그 다음에는 인덱스(숫자)로 변경되어서 버트 신경망에 인풋으로 들어게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ph2YQXg2iz5"
   },
   "outputs": [],
   "source": [
    "token_dict = {}\n",
    "with codecs.open(vocab_path, 'r', 'utf8') as reader:\n",
    "    for line in reader:\n",
    "        token = line.strip()\n",
    "        if \"_\" in token:\n",
    "          token = token.replace(\"_\",\"\")\n",
    "          token = \"##\" + token\n",
    "        token_dict[token] = len(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WhiwaBCIXqv"
   },
   "source": [
    "- Tokenizer 클래스를 상속받아서 inherit_Tokennizer라는 클래스를 재정의하고 _tokenize 함수를 새로 작성합니다. 왜 상속을 해서 함수를 새로 만들어야 하냐면, 만약 원래 클래스를 그대로 사용하면 입력한 문장이 \"완전자모분리\"됩니다. 예를 들어서 \"인공지능 할 수 있다\" 라는 문장이 있다면 원래 \"인,##공,##지,##능,할,수,있다\"로 분해가 되어야 하는데, ㅇ,##ㅣ,##ㄴ,ㄱ,##ㅗ,##ㅇ,ㅈ,##ㅣ,##ㄴ,##ㅡ,##ㅇ 이런 식으로 토큰화가 됩니다.\n",
    "\n",
    "- 두칸 아래에서 보시겠지만, inherit_Tokenizer클래스는 문장을 토큰화하는 기능을 합니다. \n",
    "\n",
    "- BERT의 토큰화는 단어를 분리하는 토큰화 방식입니다. wordpiece(단어조각?) 방식이라고 하는데, 이는 한국어를 형태소로 꼭 변환해야 할 문제를 해결해주며, 의미가 있는 단어는 밀접하게 연관이 되게 하는 장점까지 갖추고 있습니다.\n",
    "- 단어의 첫 시작은 ##가 붙지 않지만, 단어에 포함되면서 단어의 시작이 아닌 부분에는 ##가 붙는 것이 특징입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "keLv7zyZUKzZ"
   },
   "outputs": [],
   "source": [
    "class inherit_Tokenizer(Tokenizer):\n",
    "  def _tokenize(self, text):\n",
    "        if not self._cased:\n",
    "            text = text\n",
    "            \n",
    "            text = text.lower()\n",
    "        spaced = ''\n",
    "        for ch in text:\n",
    "            if self._is_punctuation(ch) or self._is_cjk_character(ch):\n",
    "                spaced += ' ' + ch + ' '\n",
    "            elif self._is_space(ch):\n",
    "                spaced += ' '\n",
    "            elif ord(ch) == 0 or ord(ch) == 0xfffd or self._is_control(ch):\n",
    "                continue\n",
    "            else:\n",
    "                spaced += ch\n",
    "        tokens = []\n",
    "        for word in spaced.strip().split():\n",
    "            tokens += self._word_piece_tokenize(word)\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88T_cMA4tsk2"
   },
   "outputs": [],
   "source": [
    "tokenizer = inherit_Tokenizer(token_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0dmxrOhJ9vD"
   },
   "source": [
    "토큰화가 잘 되었는지 확인해 봅니다.\n",
    "버트 모형은 문장 앞에 꼭 [CLS]라는 문자가 위치하고, [SEP]라는 문자가 끝에 위치합니다.  \n",
    "[CLS]는 문장의 시작, [SEP]는 문장의 끝을 의미합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-4EWrSTCMkzk"
   },
   "outputs": [],
   "source": [
    "tokenizer.tokenize(\"케라스로 버트 해보기 정말 재밌음.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fraw-fe9PL1k"
   },
   "source": [
    "우리가 로드하였던 네이버 영화 댓글 감성분석 데이터를 **버트 모형의 입력에 맞게 변형해주는 함수**를 정의합니다.\n",
    "\n",
    "함수 내부에 tokenizer.encode 함수가 버트 모형을 토큰화해주고 토큰화 된 단어를 인덱스에 맞게 숫자로 바꿔주게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8aX-OYL_X5d"
   },
   "outputs": [],
   "source": [
    "def convert_data(data_df):\n",
    "    global tokenizer\n",
    "    indices, targets = [], []\n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        ids, segments = tokenizer.encode(data_df[DATA_COLUMN][i], max_len=SEQ_LEN)\n",
    "        indices.append(ids)\n",
    "        targets.append(data_df[LABEL_COLUMN][i])\n",
    "    items = list(zip(indices, targets))\n",
    "    \n",
    "    indices, targets = zip(*items)\n",
    "    indices = np.array(indices)\n",
    "    return [indices, np.zeros_like(indices)], np.array(targets)\n",
    "\n",
    "def load_data(pandas_dataframe):\n",
    "    data_df = pandas_dataframe\n",
    "    \n",
    "    \n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
    "\n",
    "\n",
    "    data_x, data_y = convert_data(data_df)\n",
    "\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rNUYX-Rntsk_"
   },
   "outputs": [],
   "source": [
    "train_x, train_y = load_data(train)\n",
    "test_x, test_y = load_data(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXdjhy0cLlYA"
   },
   "source": [
    "- 사전학습된 버트 모델의 인풋은 문장 토큰화가 숫자로 바뀐 것과, 앞문장인지 뒷문장인지 알려주는 문장 순서 벡터가 들어갑니다. 우리는 문장 하나를 가지고만 훈련할 것이므로 순서 벡터는 모두 0으로 통일합니다.\n",
    "\n",
    "- 그리고 파인튜닝 시에는 문장 안에 일부 단어를 가리는 마스킹은 사용하지 않습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ebac6xXM1aB"
   },
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9sFKLjD-v2n"
   },
   "source": [
    "**버트 모형에 들어갈 인풋은 토큰, 세그먼트, 포지션으로 구성됩니다.**  \n",
    "버트에 인풋으로 들어가는 토큰은 문장을 토크나이징 한 후, 인덱스 번호를 매긴 것입니다.  \n",
    "세그먼트는 예를 들어 문장이 두 개가 있다면, 앞의 문장과 뒤의 문장을 구분하는 것입니다.  \n",
    "포지션 임베딩은 단순히 단어의 위치를 말합니다.\n",
    "\n",
    "토큰, 세그먼트, 포지션을 인풋으로 버트 모형에 넣으면 기하학적인 문장 공간으로 임베딩이 됩니다.\n",
    "\n",
    "![대체 텍스트](https://i.imgur.com/l9BTao3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vnh0ZuikSzz6"
   },
   "source": [
    "#### 이해가 안 가실 수 있는데, 버트 인풋을 문장으로 예를 들어 만들어 보겠습니다.\n",
    "#### 인풋은 총 2개가 들어갑니다\n",
    "- **(토큰)** 첫번째 인풋은 토큰화 된 것이 인덱싱되어 숫자로 변환된 것  \n",
    "\n",
    "- **(세그멘트)** 두번째 인풋은 앞문장인지 뒷문장인지 알려주는 숫자들입니다. 이번 튜토리얼에서는 파인튜닝 과정이라 앞문장 뒷문장 구분을 안하기 때문에 모두 0으로 하였습니다.  \n",
    "\n",
    "- **(포지션)** 단어 순서에 따라서 자동으로 부여됩니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xC-R3Be9TAG-"
   },
   "outputs": [],
   "source": [
    "def sentence_convert_data(data):\n",
    "    global tokenizer\n",
    "    indices = []\n",
    "    for i in tqdm(range(len(data))):\n",
    "        print(tokenizer.tokenize(data[i]))\n",
    "        ids, segments = tokenizer.encode(data[i], max_len=SEQ_LEN)\n",
    "        indices.append(ids)\n",
    "        \n",
    "    items = indices\n",
    "    indices = np.array(indices)\n",
    "    return [indices, np.zeros_like(indices)]\n",
    "\n",
    "def sentence_load_data(sentences):#sentence는 List로 받는다\n",
    "           \n",
    "    data_x = sentence_convert_data(sentences)\n",
    "\n",
    "    return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXKukii1Rzq3"
   },
   "outputs": [],
   "source": [
    "sentence_load_data([\"케라스로 버트 해보기 정말 재밌음\", \"케라스 쉬워 쉬워\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RZ23W0s8MCBt"
   },
   "source": [
    "이해가 되셨는지요?  \n",
    "구글 깃허브에서 다운받았던 사전학습된 모델을 colab으로 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8auSqxdHtslD",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "layer_num = 12\n",
    "model = load_trained_model_from_checkpoint(\n",
    "    config_path,\n",
    "    checkpoint_path,\n",
    "    training=True,\n",
    "    trainable=True,\n",
    "    seq_len=SEQ_LEN,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ptok2n0MHoV"
   },
   "source": [
    "모델의 구조를 확인합니다.  \n",
    "총 12층의 트랜스포머 계층이 있음을 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vBoYL3ga6A8L"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1N8hZvNMMWL"
   },
   "source": [
    "가장 중요한 부분인데요, 사전학습 파일을 로드하여, 우리가 불러들였던 사전학습 모델을 변형해줍니다.  \n",
    "  \n",
    "\n",
    "input으로는 우리가 문장을 토큰화 하여 숫자로 변형시켜주었던 **토큰 벡터와**, 앞문장인지 뒷문장인지 알려주는 **세그멘트** 두 가지가 되겠습니다.  \n",
    "  \n",
    "즉 인풋은 inputs = modle.inputs[:2]로 정의하였습니다.  \n",
    "  \n",
    "  \n",
    "그리고 output은 일단 사전학습 모델을 약간 잘라줍니다.(outputs=Dense(1)) 맨 위 3층을 잘라 낸다음에 잘라낸 부분에 긍정인지 부정인지 알려주는 **Dense(1)을 사전학습 모델에 애드온 시켜 줍니다.**  \n",
    "\n",
    "Dense(1)은 아웃풋이 하나로, **문장이 긍정에 가까우면 0에 가까운 값**을, **부정에 가까우면 1에 가까운 값**을 출력해주는 레이어입니다.\n",
    "\n",
    "\n",
    "\n",
    "그리고 사전 설치하였던 Radam을 활용하여 deep learning의 기울기 강하 훈련을 하도록 정해줍니다. 그 다음에 bert_model을 return해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ihc_AiRIm1w"
   },
   "outputs": [],
   "source": [
    "def get_bert_finetuning_model(model):\n",
    "  inputs = model.inputs[:2]\n",
    "  dense = model.layers[-3].output\n",
    "\n",
    "\n",
    "  outputs = keras.layers.Dense(1, activation='sigmoid',kernel_initializer=keras.initializers.TruncatedNormal(stddev=0.02),\n",
    "                              name = 'real_output')(dense)\n",
    "\n",
    "\n",
    "\n",
    "  bert_model = keras.models.Model(inputs, outputs)\n",
    "  bert_model.compile(\n",
    "      optimizer=RAdam(learning_rate=0.00001, weight_decay=0.0025),\n",
    "      loss='binary_crossentropy',\n",
    "      metrics=['accuracy'])\n",
    "  \n",
    "  return bert_model\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H2LAk6d5dm4A"
   },
   "source": [
    "**모델의 FLOW를 확인해 보도록 하겠습니다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "scWw12ysbPZ9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot\n",
    "\n",
    "\n",
    "SVG(model_to_dot(get_bert_finetuning_model(model), dpi=65).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gLLayijHNEjz"
   },
   "source": [
    "bert_model을 get_bert_finetuning_model 함수로 불러들입니다.\n",
    "이 다음에 bert_model.fit을 활용하여 버트 모형 파인튜닝을 시작하게 됩니다.\n",
    "실시간으로 테스트 데이터에 대한 정확도를 알기 위해 validation_data = (test_x, test_y)로 정하였습니다.\n",
    "\n",
    "**2 에포크 만에 정확도가 88.02%에 도달함을 알 수 있습니다.**  \n",
    "  \n",
    "\n",
    "만약에 ETRI에서 만든 한국어로 학습된 BERT 모형을 사용하면 정확도가 90%가 넘기도 합니다.(조만간 ETRI BERT를 활용하여 정확도 높이는 기법도 알려 드리겠습니다.)  \n",
    "  \n",
    "\n",
    "사실 아무 전처리 없이 네이버 감성분석의 정확도가 2 에포크 만에 88.02%가 나온 것은 상당히 고무적인 것입니다.  \n",
    "  \n",
    "\n",
    "BiLSTM 이나 어텐션 기법을 사용했을 시 보통 테스트셋에 대한 정확도가 86% 정도로 알려져 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7On1hY3I7o1"
   },
   "outputs": [],
   "source": [
    "sess = K.get_session()\n",
    "uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n",
    "init = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables])\n",
    "sess.run(init)\n",
    "\n",
    "bert_model = get_bert_finetuning_model(model)\n",
    "history = bert_model.fit(train_x, train_y, epochs=2, batch_size=16, verbose = 1, validation_data=(test_x, test_y), shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-HySrkKNV_e"
   },
   "source": [
    "재사용을 위해 bert_model을 지드라이브에 저장해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E3pc5zhjCet-"
   },
   "outputs": [],
   "source": [
    "bert_model.save_weights(path+\"/bert.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0ZgfR6MRIul"
   },
   "source": [
    "버트 모형을 로드해줍니다. 이미 로드하였던 모델에 계수들만 살짝 얹혀 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DwMLCYp0TzTv"
   },
   "outputs": [],
   "source": [
    "bert_model = get_bert_finetuning_model(model)\n",
    "bert_model.load_weights(path+\"/bert.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqkDeYdwUC5x"
   },
   "source": [
    "파인튜닝한 버트 모형에 test 데이터 셋을 넣어 평가해 봅니다.\n",
    "사실 머신러닝에서는 정확도인 accuracy도 중요하지만, **F1 score**가 상당히 중요합니다.\n",
    "  \n",
    "\n",
    "긍정문장으로 판별된 문장이 진짜로 **긍정 문장에 속할 확률**  \n",
    "  \n",
    "\n",
    "부정문장으로 판별된 문장이 진짜로 **부정 문장에 속할 확률**  \n",
    " **이 두가지를 고려한 종합적인 성능이 F1 score입니다.**\n",
    "\n",
    "한번 F1-score를 확인해 보겠습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-lkxG0_Ul-_"
   },
   "source": [
    "먼저 테스트 데이터를 버트 모형에 넣을 수 있도록 predict_convert_data 함수를 정의해줍니다. <br> 위에 정의한 convert_data 함수와 다른 점은, test 데이터이기 때문에 label은 고려하지 않는다는 점입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ssOqKQeQXOe2"
   },
   "outputs": [],
   "source": [
    "def predict_convert_data(data_df):\n",
    "    global tokenizer\n",
    "    indices = []\n",
    "    for i in tqdm(range(len(data_df))):\n",
    "        ids, segments = tokenizer.encode(data_df[DATA_COLUMN][i], max_len=SEQ_LEN)\n",
    "        indices.append(ids)\n",
    "        \n",
    "    items = indices\n",
    "    \n",
    "    \n",
    "    indices = np.array(indices)\n",
    "    return [indices, np.zeros_like(indices)]\n",
    "\n",
    "def predict_load_data(x): #Pandas Dataframe을 인풋으로 받는다\n",
    "    data_df = x\n",
    "    \n",
    "    \n",
    "    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n",
    "\n",
    "\n",
    "    data_x = predict_convert_data(data_df)\n",
    "\n",
    "    return data_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7OGi5CQpVE0S"
   },
   "outputs": [],
   "source": [
    "test_set = predict_load_data(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ON1vhhIVWWj"
   },
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiHzDNCxVXPA"
   },
   "source": [
    "테스트 셋으로 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mhvOT0wiVNmj"
   },
   "outputs": [],
   "source": [
    "#예측\n",
    "preds = bert_model.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PcLzb1iuWeCg"
   },
   "outputs": [],
   "source": [
    "# 부정이면 0, 긍정이면 1 출력\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fq3GjEB8Vr0d"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nBwXhtRHVs1I"
   },
   "outputs": [],
   "source": [
    "y_true = test['label']\n",
    "# F1 Score 확인\n",
    "print(classification_report(y_true, np.round(preds,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VfY5HbAWr1W"
   },
   "source": [
    "F1 스코어와 Accuracy가 거의 비슷함을 알 수 있습니다. 훈련이 치우치지 않고 잘 됐음을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "04FC9NuAeyHA"
   },
   "source": [
    "#### 한번 케라스의 장점을 살려 볼까요?\n",
    "#### 케라스의 장점은 모델을 쉽게 자르고 붙일 수 있다는 점 같습니다\n",
    "#### 마지막 768개의 피처 부분을 잘라내서, 긍정 영화 평가와 부정 영화 평가의  플롯을 한번 그려보고 상호간 기하학적 관계를 살펴보도록 하겠습니다.\n",
    "\n",
    "#### 먼저, 마지막 768개 피처를 추출하는 함수를 정의합니다\n",
    "#### 이번에는 아웃풋이 0과 1 사이의 값이 아닌, 768개의 값을 반환하게 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9otQ50QW86J"
   },
   "outputs": [],
   "source": [
    "def get_feature_map(model):\n",
    "  inputs = model.input\n",
    "  outputs = model.layers[-2].output\n",
    "  feature_model = Model(inputs, outputs)\n",
    "  return feature_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "executionInfo": {
     "elapsed": 758,
     "status": "error",
     "timestamp": 1601723973197,
     "user": {
      "displayName": "정인환",
      "photoUrl": "",
      "userId": "14508790529200005254"
     },
     "user_tz": -540
    },
    "id": "zDS4FulWXOhP",
    "outputId": "1a7fdf1d-7c1d-4690-c309-5a3f1f340587"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-1a18178910ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbert_feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_feature_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_model' is not defined"
     ]
    }
   ],
   "source": [
    "bert_feature = get_feature_map(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oAPWfGQnYY9v"
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(bert_feature, dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-7-waW9fKzw"
   },
   "source": [
    "마지막 부분을 보시면 768개의 피처가 반환되게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6P2l2NOBfQiv"
   },
   "source": [
    "테스트 데이터의 피처들을 반환하고, TSNE 임베딩 플롯을 그려보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikYOtbmQdhaX"
   },
   "outputs": [],
   "source": [
    "bert_weight_list = bert_feature.predict(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5n-PKTqQYDRA"
   },
   "outputs": [],
   "source": [
    "bert_weight_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aB11e70pdJvB"
   },
   "outputs": [],
   "source": [
    "labels = test['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hpwPgF8yYUXT"
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uW8XrJVZ2cO5"
   },
   "source": [
    "마지막 768 차원을 PCA를 사용하여 256차원으로 줄여줍니다.\n",
    "그 다음 TSNE 알고리즘을 활용하여 3차원으로 축소해 줍니다.\n",
    "TSNE 알고리즘은 유사한 것끼리 클러스터를 만들어 주는 역할을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oKFPLdGjZBZ-"
   },
   "outputs": [],
   "source": [
    "bert_embedded = PCA(n_components=256).fit_transform(bert_weight_list)\n",
    "bert_embedded = TSNE(n_components=3).fit_transform(bert_embedded)\n",
    "bert_embedded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kx5JEVVi20kw"
   },
   "source": [
    "bert 임베딩을 pickle 모듈을 활용하여 저장해 줍니다.  \n",
    "\n",
    "추후 재사용에 쓸 예정입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V-byBcHs_A6r"
   },
   "outputs": [],
   "source": [
    "with open(path+\"/bertembedding.pkl\", \"wb\") as f:\n",
    "  pickle.dump(bert_embedded, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uetig7oH279N"
   },
   "source": [
    "저장하였던 임베딩 파일을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KwaKQNtgqS6g"
   },
   "outputs": [],
   "source": [
    "with open(path+\"/bertembedding.pkl\", \"rb\") as f:\n",
    "  bert_embedded = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gil3Ja8R2_xz"
   },
   "source": [
    "3D로 그리기 위해 matplotlib 모듈들을 로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vf0rvWOScDrf"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import animation\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aIqz-_jMsSX4"
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qMENr8gA1R0O"
   },
   "outputs": [],
   "source": [
    "def get_tsne_plot(rot1=-20, rot2=100):\n",
    "  fig = plt.figure(figsize=(5,5))\n",
    "  ax = Axes3D(fig)\n",
    "  colors = 'b', 'r'\n",
    "  labels = 0, 1\n",
    "  for i, c, label in zip(range(np.shape(bert_embedded)[0]), colors, labels):\n",
    "    ax.scatter(bert_embedded[test['label']==label,0], bert_embedded[test['label']==label,1], bert_embedded[test['label']==label,2], s=2, c=c, alpha=0.5)\n",
    "  ax.view_init(rot1, rot2)\n",
    "  print(\"rot1:%d\" % rot1, \"rot2:%d\" % rot2)\n",
    "  plt.legend(labels, loc='upper right')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "avTvJ4-r38q1"
   },
   "source": [
    "긍정(1)과 부정(0)의 임베딩이 잘 되는지 확인해 봅니다.  \n",
    "\n",
    "긍정 부분의 클러스터와 부정구간의 클러스터가 잘 구분되는 것을 확인하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PWdJgsgIcCb5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "for j in range(-180, 180, 45):\n",
    "  for i in range(-180,180,45):\n",
    "  \n",
    "    get_tsne_plot(i, j)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i68Mc4PRPW3X"
   },
   "source": [
    "한번 문장을 입력하면 긍정인지 부정인지 알려주는 함수를 만들어 볼까요?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CPaJVx6rKvhT"
   },
   "outputs": [],
   "source": [
    "def sentence_convert_data(data):\n",
    "    global tokenizer\n",
    "    indices = []\n",
    "    ids, segments = tokenizer.encode(data, max_len=SEQ_LEN)\n",
    "    indices.append(ids)\n",
    "        \n",
    "    items = indices\n",
    "    indices = np.array(indices)\n",
    "    return [indices, np.zeros_like(indices)]\n",
    "\n",
    "def movie_evaluation_predict(sentence):\n",
    "    data_x = sentence_convert_data(sentence)\n",
    "    predict = bert_model.predict(data_x)\n",
    "    predict_answer = np.round(np.ravel(predict), 0).item()\n",
    "    \n",
    "    if predict_answer == 0:\n",
    "      print(\"부정적인 영화 평가입니다.\")\n",
    "    elif predict_answer == 1:\n",
    "      print(\"긍정적인 영화 평가입니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oTZXaaXkLb0n"
   },
   "outputs": [],
   "source": [
    "movie_evaluation_predict(\"나만 이걸 보고 울었는지 모르겠지만 우리나라의 역사의 슬픔과 왕과 신하의 신뢰와 믿음... 그 모든것이 절 슬프게 하였네요... \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOddZMJHLdR4"
   },
   "outputs": [],
   "source": [
    "movie_evaluation_predict(\"너무잼있어엉 진짜 연기가 예술이고 다시보고싶은영화\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2i-gZfPlM5Hn"
   },
   "outputs": [],
   "source": [
    "movie_evaluation_predict(\"영상미는 좋지만, 스토리와 연출이 너무너무 진부해서 하품100번하다 나왔네요..\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iigCLHMBN_Pq"
   },
   "outputs": [],
   "source": [
    "movie_evaluation_predict(\"배우들이 맞지 않는 옷을 입은 것처럼 연기력 대부분이 별로였습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZWTWYKOuOJ_B"
   },
   "outputs": [],
   "source": [
    "movie_evaluation_predict(\"평범한 스토리. 볼만한 영상미. 스타워즈도 이제는...\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03_케라스로_버트_빠르게_돌려보기_With_네이버_영화_감성분석_TUTORIAL.ipynb",
   "provenance": [
    {
     "file_id": "12SmAnH0KZH_gjOHJqHVS26gla5vah4dr",
     "timestamp": 1601724324614
    },
    {
     "file_id": "1vLz41ShWRtJGw7yC-A1AL3fBU-24MMVU",
     "timestamp": 1601722632205
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
