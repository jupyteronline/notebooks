{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/jupyteronline/notebooks/blob/master/6_nlp/02_자연어처리_News_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSZaDxZZxk9g"
   },
   "source": [
    "# 뉴스 카테고리 분류하기  \n",
    "본 튜토리얼에서는 사전학습된 임베딩을 활용하여 뉴스의 카테고리를 분류하는 실습을 하고자 합니다.  \n",
    "https://www.kaggle.com/rmisra/news-category-dataset 의 자료를 참조하였습니다.  \n",
    "  \n",
    "![imgur](https://i.imgur.com/bp9eNMS.jpg)  \n",
    "  \n",
    "  헤드라인과 기사요약 칼럼을 활용해서, 뉴스 category를 맞추는 문제입니다.  \n",
    "  category는 40종으로 이루어져 있어서 생각보다 어려운 문제입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bWpjD5GyLlPH"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import backend as K\n",
    "from keras.layers import Embedding, Dense, Input, LSTM, Bidirectional, Activation, Conv1D, GRU, TimeDistributed, Dropout\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQ8cvTXLLoQ-"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-L7kbwXaL8dm"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0gHErKAMfq0"
   },
   "outputs": [],
   "source": [
    "os.listdir('gdrive/My Drive/Colab Notebooks/education')   # https://github.com/nomadotto/News_Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FyXB2-2YNK3c"
   },
   "outputs": [],
   "source": [
    "path = 'gdrive/My Drive/Colab Notebooks/education'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKxIxTRQ1DL9"
   },
   "source": [
    "뉴스 데이터를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A1qVHHE3M-qJ"
   },
   "outputs": [],
   "source": [
    "data = pd.read_json(os.path.join(path,'News_Category_Dataset_v2.json'), lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzMOMpQ4NG5J"
   },
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgULLhYB1HmK"
   },
   "source": [
    "데이터는 200,853개의 뉴스와, 7개의 칼럼으로 이루어져 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_CBg_fhJ1FU6"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GSy8dfKlNU6F"
   },
   "outputs": [],
   "source": [
    "data['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJsoqc3r1LtH"
   },
   "source": [
    "우리가 예측해야 하는 것은 category인데요, 잘 보시면 WORLDPOST와 THE WORLDPOST가 실제로는 같은 카테고리인데 나뉘어져 있음을 알 수 있습니다.  \n",
    "pandas의 map 함수를 활용하여 WORLDPOST와 THE WORLDPOST를 합쳐줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bfehwf2NdsV"
   },
   "outputs": [],
   "source": [
    "data['category'] = data['category'].map(lambda x: \"WORLDPOST\" if x == 'THE WORLDPOST' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KzI_jopINt53"
   },
   "outputs": [],
   "source": [
    "data['category'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wF10moMu1gVw"
   },
   "source": [
    "headline 데이터와 short_description 데이터를 합쳐줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Zdr3waoOB9J"
   },
   "outputs": [],
   "source": [
    "data['text'] = data['headline'] + \" \" + data['short_description']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc4K0e4a1j6Q"
   },
   "source": [
    "IMDB 데이터와 다르게 자연어 처리 모듈인 NLTK를 활용하여 단어 토큰화를 진행하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx_PuIFSOKk6"
   },
   "outputs": [],
   "source": [
    "#NLTK TOKENIZER 사용하기\n",
    "import nltk\n",
    "nltk.download(\"book\", quiet=True)\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAbDiq9z1uR7"
   },
   "source": [
    "정규식을 사용해서 문장에서 숫자와 알파벳이 아닌 단어들은 제외해주겠습니다.  \n",
    "그리고 문장을 단어로 구성된 리스트로 바꿔 주도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6jqdCHNQOnK0"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "retokenize = RegexpTokenizer(\"[\\w]+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rOds4lryPL4S"
   },
   "outputs": [],
   "source": [
    "text_list = []\n",
    "for sentence in data['text'].tolist():\n",
    "  text_list.append(retokenize.tokenize(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-7AAlw_H11rI"
   },
   "source": [
    "문장이 단어들로 분해되었음을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oh4AXWkZPu2U"
   },
   "outputs": [],
   "source": [
    "print(text_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tu-NTXJE2PhV"
   },
   "source": [
    "영어 단어에서, 어간을 추출해보도록 하겠습니다.  \n",
    "예를 들어서 shooting이나 shoot이나 의미는 비슷할 것입니다.  \n",
    "이러한 변형 형태를 하나로 만들어주는 어간을 nltk를 활용하여 추출하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BKaNi7jPy7_"
   },
   "outputs": [],
   "source": [
    "# 어간 추출하기\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "st = PorterStemmer()\n",
    "\n",
    "stem_text_list = []\n",
    "for sentence in tqdm(text_list):\n",
    "    stem_text_list.append([st.stem(w) for w in sentence if w not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ei5pqZap2fln"
   },
   "outputs": [],
   "source": [
    "print(text_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXqT26Ib2hMg"
   },
   "source": [
    "위의 단어와 비교해 보면 shootings는 shoot으로, killed는 kill 로, 바뀌었음을 알 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3AbAZUXXQPQa"
   },
   "outputs": [],
   "source": [
    "print(stem_text_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Pe9e0MpQSUq"
   },
   "outputs": [],
   "source": [
    "# 단어들에 넘버링 하기\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(stem_text_list)\n",
    "x_train = tokenizer.texts_to_sequences(stem_text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2RM1pyfRZgP"
   },
   "outputs": [],
   "source": [
    "print('max length :',max(len(l) for l in x_train))\n",
    "print('average length :',sum(map(len, x_train))/len(x_train))\n",
    "plt.hist([len(s) for s in x_train], bins=50)\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KAga9bhda2_v"
   },
   "outputs": [],
   "source": [
    "max_len = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcWqomDGRkMi"
   },
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "y_train = np.array(data['category'].factorize()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JzN0QWanS2Dg"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdzjLpPp2qt6"
   },
   "source": [
    "gensim 모듈을 활용하여 사전학습된 언어 모델을 불러옵니다.  \n",
    "glove는 word2vec 이후에 나온 사전학습된 언어 모델입니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3CMZhdJNTEwV"
   },
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxhNNp0GX6rB"
   },
   "outputs": [],
   "source": [
    "import os, requests, shutil\n",
    "\n",
    "glove_dir = './glove'\n",
    "glove_100k_50d = 'glove.first-100k.6B.50d.txt'\n",
    "glove_100k_50d_path = os.path.join(glove_dir, glove_100k_50d)\n",
    "\n",
    "# These are temporary files if we need to download it from the original source (slow)\n",
    "data_cache = './data/cache'\n",
    "glove_full_tar = 'glove.6B.zip'\n",
    "glove_full_50d = 'glove.6B.50d.txt'\n",
    "\n",
    "#force_download_from_original=False\n",
    "download_url= 'http://redcatlabs.com/downloads/deep-learning-workshop/notebooks/data/RNN/'+glove_100k_50d\n",
    "original_url = 'http://nlp.stanford.edu/data/'+glove_full_tar\n",
    "\n",
    "if not os.path.isfile( glove_100k_50d_path ):\n",
    "    if not os.path.exists(glove_dir):\n",
    "        os.makedirs(glove_dir)\n",
    "    \n",
    "    # First, try to download a pre-prepared file directly...\n",
    "    response = requests.get(download_url, stream=True)\n",
    "    if response.status_code == requests.codes.ok:\n",
    "        print(\"Downloading 42Mb pre-prepared GloVE file from RedCatLabs\")\n",
    "        with open(glove_100k_50d_path, 'wb') as out_file:\n",
    "            shutil.copyfileobj(response.raw, out_file)\n",
    "    else:\n",
    "        # But, for some reason, RedCatLabs didn't give us the file directly\n",
    "        if not os.path.exists(data_cache):\n",
    "            os.makedirs(data_cache)\n",
    "        \n",
    "        if not os.path.isfile( os.path.join(data_cache, glove_full_50d) ):\n",
    "            zipfilepath = os.path.join(data_cache, glove_full_tar)\n",
    "            if not os.path.isfile( zipfilepath ):\n",
    "                print(\"Downloading 860Mb GloVE file from Stanford\")\n",
    "                response = requests.get(download_url, stream=True)\n",
    "                with open(zipfilepath, 'wb') as out_file:\n",
    "                    shutil.copyfileobj(response.raw, out_file)\n",
    "            if os.path.isfile(zipfilepath):\n",
    "                print(\"Unpacking 50d GloVE file from zip\")\n",
    "                import zipfile\n",
    "                zipfile.ZipFile(zipfilepath, 'r').extract(glove_full_50d, data_cache)\n",
    "\n",
    "        with open(os.path.join(data_cache, glove_full_50d), 'rt') as in_file:\n",
    "            with open(glove_100k_50d_path, 'wt') as out_file:\n",
    "                print(\"Reducing 50d GloVE file to first 100k words\")\n",
    "                for i, l in enumerate(in_file.readlines()):\n",
    "                    if i>=100000: break\n",
    "                    out_file.write(l)\n",
    "    \n",
    "        # Get rid of tarfile source (the required text file itself will remain)\n",
    "        #os.unlink(zipfilepath)\n",
    "        #os.unlink(os.path.join(data_cache, glove_full_50d))\n",
    "\n",
    "print(\"GloVE available locally\")\n",
    "\n",
    "def loadGloveModel(gloveFile):\n",
    "    print(\"Loading Glove Model\")\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "        model[word] = embedding\n",
    "    print(\"Done.\",len(model),\" words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQ0YKkT4Z4w-"
   },
   "outputs": [],
   "source": [
    "word_embedding = loadGloveModel(glove_100k_50d_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETxt9JPK23gf"
   },
   "source": [
    "단어 하나 하나가, 50개의 벡터로 임베딩 되었음을 보실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5FeF5rXZ-0u"
   },
   "outputs": [],
   "source": [
    "word_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0oMus1X_JWX"
   },
   "source": [
    "사전 임베딩된 모델에서, 우리가 문장에 있는 데이터만 가지고서 embedding matrix를 만듭니다.  \n",
    "설명드리자면, 사전 임베딩된 모델은 우리가 문장에 있는 단어뿐만아니라 외부에 있는 단어(wikipedia)같은 곳에 있는 것들로도 학습이 된 것입니다.  \n",
    "따라서 우리가 훈련에 필요한 단어의 임베딩만 가져오는 작업이 필요합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgYIdgxiUzSY"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 50))\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LpX3B6LBVvER"
   },
   "outputs": [],
   "source": [
    "def get_vector(word):\n",
    "    if word in word_embedding:\n",
    "        return word_embedding[word]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    temp = get_vector(word)\n",
    "    if temp is not None:\n",
    "        embedding_matrix[i] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DIK8mohq_spt"
   },
   "source": [
    "**category는** array(['CRIME', 'ENTERTAINMENT', 'WORLD NEWS', 'IMPACT', 'POLITICS',\n",
    "       'WEIRD NEWS', 'BLACK VOICES', 'WOMEN', 'COMEDY', 'QUEER VOICES',\n",
    "       'SPORTS', 'BUSINESS', 'TRAVEL', 'MEDIA', 'TECH', 'RELIGION',\n",
    "       'SCIENCE', 'LATINO VOICES', 'EDUCATION', 'COLLEGE', 'PARENTS',\n",
    "       'ARTS & CULTURE', 'STYLE', 'GREEN', 'TASTE', 'HEALTHY LIVING',\n",
    "       'WORLDPOST', 'GOOD NEWS', 'FIFTY', 'ARTS', 'WELLNESS', 'PARENTING',\n",
    "       'HOME & LIVING', 'STYLE & BEAUTY', 'DIVORCE', 'WEDDINGS',\n",
    "       'FOOD & DRINK', 'MONEY', 'ENVIRONMENT', 'CULTURE & ARTS']  \n",
    "        **같은 문자로 이루어져 있습니다. 이 문자들을 숫자들로 변경해줍니다**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vX8G993cqZx"
   },
   "outputs": [],
   "source": [
    "data['category'] = data['category'].factorize()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vJtmokELczfm"
   },
   "outputs": [],
   "source": [
    "data['category'].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiK39HOabQo4"
   },
   "outputs": [],
   "source": [
    "int_category = data['category'].unique().shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K2JOwP5j2_Fn"
   },
   "source": [
    "어텐션 모형을 사용하도록 하겠습니다.  \n",
    "RNN, LSTM의 단점은, 문장이 길어질수록 훈련 과정에서 gradient가 소실되는 경향이 있습니다. 그래서 모델의 성능을 저하하기도 합니다.  \n",
    "  \n",
    "  **어텐션 모형은 LSTM 알고리즘 적용 후, 훈련 과정에서 한번 더 전의 학습 과정을 복습하는 효과를 가지게 됩니다.** 사실 유명한 모형인 BERT 모형도 어텐션 층으로 이루어진 모델을 가지고 있습니다.  \n",
    "![imgur](https://i.imgur.com/MGgDWxi.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCISlVhXbf9F"
   },
   "outputs": [],
   "source": [
    "!pip install keras-self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XJI-DJsyb4mR"
   },
   "outputs": [],
   "source": [
    "from keras_self_attention import SeqWeightedAttention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViwEKJBh_9Vi"
   },
   "source": [
    "이번에는 sparse_categorical_crossentropy를 사용하도록 하겠습니다.  \n",
    "보통 categorical_crossentropy 같은 경우에는 만약에 라벨이 0,1,2 라면, [1,0,0], [0,1,0], [0,0,1]로 변환하는 작업이 필요합니다.  \n",
    "sparse_categorical_crossentropy 같은 경우에는 라벨을 변환할 필요 없이 훈련에 라벨을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8kxNGRbWM2h"
   },
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 50, weights=[embedding_matrix], input_length=max_len, trainable=False)\n",
    "inputs = Input(shape=(max_len,), dtype='int32')\n",
    "embedding= e(inputs)\n",
    "x = Bidirectional(LSTM(50, return_sequences=True))(embedding)\n",
    "merged = SeqWeightedAttention()(x) #attention layer 추가\n",
    "merged = Dense(80, activation='relu')(merged)\n",
    "merged = Dropout(0.25)(merged)\n",
    "\n",
    "outputs = Dense(int_category, activation='softmax')(merged)\n",
    "\n",
    "attention_model = Model(inputs=inputs, outputs=outputs)\n",
    "attention_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['sparse_categorical_accuracy'])\n",
    "\n",
    "attention_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XUy9Qt1BFqp"
   },
   "source": [
    "모델의 FLOW를 그림으로 나타내 보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLSqlZK9e6AH"
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils import model_to_dot\n",
    "SVG(model_to_dot(attention_model, dpi=65).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uC16pH5JcQo4"
   },
   "outputs": [],
   "source": [
    "history = attention_model.fit(x_train, y_train, batch_size = 2048, validation_split=0.1, shuffle=True, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LhhiQi6-uurT"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['sparse_categorical_accuracy']) + 1)\n",
    "plt.plot(epochs, history.history['sparse_categorical_accuracy'])\n",
    "plt.plot(epochs, history.history['val_sparse_categorical_accuracy'])\n",
    "plt.title('Training')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JO0Zx5Rt-uif"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "02_자연어처리_News_Classification.ipynb의 사본",
   "provenance": [
    {
     "file_id": "1sGQktnCcdeMFOd8XdQhSi9ZFPKAUc4j7",
     "timestamp": 1601724520632
    },
    {
     "file_id": "1SI-Bg_I1DtXhozGw4UWKG3yXBXQZD1HR",
     "timestamp": 1601724266940
    },
    {
     "file_id": "1ety7yQ_lqOaudli7Pt1rT-yzIHxAkZ2H",
     "timestamp": 1601720901280
    },
    {
     "file_id": "1c0RIyNsCqTOy7rfQWVKEH1mAb2-hKfsI",
     "timestamp": 1601720858398
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
