{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "# <a href=\"https://colab.research.google.com/github/jupyteronline/notebooks/blob/master/6_nlp/01_%EC%9E%90%EC%97%B0%EC%96%B4%EC%B2%98%EB%A6%AC_%EA%B8%B0%EC%B4%88(IMDB_DATASET).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cMsiZxTjl3re"
   },
   "source": [
    "# 딥러닝을 위한 자연어 기초  \n",
    "  \n",
    "딥러닝을 활용하여 텍스트를 분류하기 위해서는  \n",
    "**1) 토큰화, 2) 단어를 숫자로 변환하는 넘버링, 3) 1차원의 단어를 고차원으로 임베딩, 4) RNN이나 LSTM 같은 모형을 적용**하여 분류하는 4단계 과정을 거치게 됩니다,  \n",
    "  \n",
    "  \n",
    "\n",
    "- **(토큰화)** 토큰화는 연속된 문장을 단어 단위로 잘라주는 것입니다.  \n",
    "  \n",
    "\n",
    "\n",
    "- **(넘버링)** 단어마다 순서를 정해 주면 다루기 상당히 편합니다.  \n",
    "  \n",
    "\n",
    "\n",
    "- **(임베딩)** 순서가 부여된 단어들은 컴퓨터 입장에서는 연관성을 알 수 없는 숫자들에 불과하다. 따라서 여러 문장을 훈련하여 단어간의 연관성을 부여해준다. 이 과정을 임베딩이라고 한다. 넘버링된 한 개의 숫자가 여러개의 숫자로 벡터화되게 됩니다.  \n",
    "  \n",
    "- **(RNN 적용)** 토큰화되고, 토큰화된 단어들이 순서가 부여된 후 임베딩된 문장들은 문장 하나 하나가 RNN 모형에 들어가서 훈련되게 되며, 최종적으로 문장을 분류하는 AI가 만들어지게 됩니다.\n",
    "\n",
    "    본 튜토리얼에서는 자연어 처리의 기초를 배울 겸 imdb에 있는 영화 리뷰를 바탕으로 감성분석을 해보려고 합니다.\n",
    "\n",
    "![Imgur](https://i.imgur.com/YFiEvOV.jpg)  \n",
    "  \n",
    "\n",
    "\n",
    "![Imgur](https://i.imgur.com/9Z2HwRx.jpg)  \n",
    "  \n",
    "\n",
    "\n",
    "![Imgur](https://i.imgur.com/ZABQd8V.jpg)  \n",
    "  \n",
    "  \n",
    "\n",
    "![Imgur](https://i.imgur.com/70HAAbF.jpg)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x5T455iQ2Pe4"
   },
   "outputs": [],
   "source": [
    "%tensorflow_version 1.x\n",
    "# IMDB 데이터셋 로드\n",
    "from keras.datasets import imdb\n",
    "\n",
    "# Numpy, Pandas, Matplotlib 로드\n",
    "import numpy as np # 파이썬에서 수치를 다루기 위한 모듈\n",
    "import pandas as pd # 파이썬에서 table을 다루기 위한 모듈\n",
    "import matplotlib.pyplot as plt # 파이썬에서 그림을 그리기 위한 모듈\n",
    "import tensorflow as tf\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9vZ9Pg4Il3E"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import tensorflow as tf\n",
    "warnings.filterwarnings(action='ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ULgFKqsPtYuR"
   },
   "source": [
    "imdb 데이터셋을 로드하도록 하겠습니다.  \n",
    "num_words = 20000은, 문장에 등장하는 단어들의 집합에서, 단어 등장 빈도수가 상위 20,000번째까지만 불러옵니다  \n",
    "딥러닝 훈련에 쓸 train 데이터와 훈련된 딥러닝의 성능을 검증할 test 데이터를 불러옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "a_-4lElcFi11",
    "outputId": "b914c284-b7ee-48bc-b955-3deefc349077"
   },
   "outputs": [],
   "source": [
    "help(imdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "IXUROgNB4JIo",
    "outputId": "85eac657-b426-4407-f3fa-5b30b06d77b5"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aqunV0Y8tg3Z"
   },
   "source": [
    "imdb 데이터셋에서 사용되는 단어와 그 단어에 해당하는 숫자가 맵핑되어 있는 딕셔너리를 불러옵니다. {단어:숫자, 단어:숫자,.....}  \n",
    "또한 reverse_word_dict를 정의하여 {숫자:단어, 숫자:단어, ...} 형식으로도 만들어줍니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "bV9bH1-K4Uxz",
    "outputId": "7da699fc-7e41-42e2-f9b3-97b37b1e9126"
   },
   "outputs": [],
   "source": [
    "word_dict = imdb.get_word_index()\n",
    "reverse_word_dict = {k:v for v, k in word_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "KnTDpy0RD_m3",
    "outputId": "dbaacd15-9221-40a5-cf6f-7ca5e419a1fa"
   },
   "outputs": [],
   "source": [
    "word_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bsCpDh32wE-C"
   },
   "source": [
    "단어(이미 토큰화되어있음)와 숫자가 서로 매핑된 것을 볼 수 있다.  \n",
    "이번 데이터셋은 별도의 토큰화 과정을 거치지 않도록 전처리가 되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 91
    },
    "colab_type": "code",
    "id": "QYrv8ggH5huc",
    "outputId": "2a311507-a17b-4e5c-a8ab-dd75425a3a39"
   },
   "outputs": [],
   "source": [
    "for i in x_train[0]:\n",
    "    \n",
    "    print(reverse_word_dict[i], end = \" \")\n",
    "print(\"\\n\")\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8cyAzh45E3oo",
    "outputId": "344a7ca1-a68c-4d0f-fe53-d1596de7b39c"
   },
   "outputs": [],
   "source": [
    "for sent in x_train[0:50]:\n",
    "  for i in sent:\n",
    "    \n",
    "      print(reverse_word_dict[i], end = \" \")\n",
    "  print(\"\\n\")\n",
    "  print(x_train[0])\n",
    "  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NDNAK-aawW6b"
   },
   "source": [
    "문장의 최대 길이, 평균 길이가 얼마나 되는지 알아본다  \n",
    "문장의 최대 길이는 2494, 평균 길이는 238 정도임을 알 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "colab_type": "code",
    "id": "G1l2CdPbM2-V",
    "outputId": "ddb25575-ed58-43d2-c130-9833c3628461"
   },
   "outputs": [],
   "source": [
    "print('maximun length : {}'.format(max(len(l) for l in x_train)))\n",
    "print('average length : {}'.format(sum(map(len, x_train))/len(x_train)))\n",
    "\n",
    "plt.hist([len(s) for s in x_train], bins=50)\n",
    "plt.xlabel('length')\n",
    "plt.ylabel('number')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4kTnQYE3Ejxw"
   },
   "source": [
    "vocab_size는 imdb 데이터가 가지고 있는 단어의 개수이다.  \n",
    "num_words를 20,000으로 했기 때문에 총 단어 개수는 20,000개이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M5QUZQyV8T8s"
   },
   "outputs": [],
   "source": [
    "vocab_size = 20000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lF6R4tuUEy-z"
   },
   "source": [
    "imdb 데이터셋은 영화 리뷰가 positive인지, negative인지만 알면 된다.  \n",
    "0인지 1인지만 구분하면 되는 binary classification 문제이다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "arUMN7Mw6HBo",
    "outputId": "6ce1a38f-a31b-4823-d476-16c4dff45fd2"
   },
   "outputs": [],
   "source": [
    "unique, bins = np.unique(y_train, return_counts=True)\n",
    "print(unique)\n",
    "print(bins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U6RvHCFEGknv"
   },
   "source": [
    "케라스를 활용하기 위해서, 필요한 모듈들을 로드한다.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SfTsRxTx7NH4"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding, Flatten, Dropout, GRU\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vVUMKhh7GwXw"
   },
   "source": [
    "문장의 최대 길이는 2494, 평균 길이는 238 정도였다.  \n",
    "max_len 변수를 지정해서 문장의 최대 길이를 500으로 맞춰 주도록 하겠다.  \n",
    "만약 500보다 긴 문장은 문장이 절단될 것이며, 500보다 짧은 문장은 길이를 500으로 맞추기 위해 0으로 채워지게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zk05i2PR7l-z"
   },
   "outputs": [],
   "source": [
    "max_len = 500\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xh6YVGbHkeO"
   },
   "source": [
    "훈련할 모델을 정의한다.\n",
    "LSTM 대신에 LSTM과 성능은 비슷하면서도 속도는 빠른 GRU 모델을 사용하겠습니다.  \n",
    "이진 분류이기 때문에 binary_crossentropy 방법으로 loss를 정의하고, adam optimization 방식으로 훈련을 시작하겠습니다.  \n",
    "Embedding(vocab_size, 100) 이라는 레이어가 있는데, 우리가 imdb 데이터에 사용되는 총 20,000개의 문장을 단어 하나 하나마다 100차원의 임베딩하겠다는 레이어를 뜻합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "colab_type": "code",
    "id": "7CndHzUu8h-I",
    "outputId": "c544fcaa-f4c7-4b39-cf57-d909cf5574c5"
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 100))\n",
    "model.add(GRU(100))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "BCdqtjnVHLav",
    "outputId": "e74d7fde-791c-4f15-b20a-a59a8ff0f1cd"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=1024, epochs=3, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9w5YsqB5MDJi"
   },
   "source": [
    "훈련 과정을 그래프로 나타내어 봅니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "th-Pj70TJiVE",
    "outputId": "20e4c913-a877-410f-d400-26daec19c696"
   },
   "outputs": [],
   "source": [
    "epochs = range(1, len(history.history['acc']) + 1)\n",
    "plt.plot(epochs, history.history['acc'])\n",
    "plt.plot(epochs, history.history['val_acc'])\n",
    "plt.title('Training')\n",
    "plt.ylabel('acc')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZoKSJ6nBMQLO"
   },
   "source": [
    "F1 SCORE 확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwb9Ee71L975"
   },
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "preds = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "BUxKdBrpMXSE",
    "outputId": "d57991a0-3da9-4767-d96f-5fd1840d4ef1"
   },
   "outputs": [],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "YKOc5_7KMleK",
    "outputId": "743c69f6-426c-456c-8718-b0d21a883873"
   },
   "outputs": [],
   "source": [
    "np.round(np.ravel(preds),0), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "colab_type": "code",
    "id": "xpTRJX3kMYYd",
    "outputId": "8731aaf6-8c07-4f36-8399-372c84e43695"
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_test, np.round(np.ravel(preds),0)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "01_자연어처리_기초(IMDB_DATASET).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
