{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"04_(개정)케라스로_Q&A_구현하기_With_SQUAD.ipynb의 사본","provenance":[{"file_id":"1geuf6fg1kdxk5pSGI1A4up5mi9Ilk55i","timestamp":1601724383195}],"collapsed_sections":["vnh0ZuikSzz6"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/jupyteronline/notebooks/blob/master/6_nlp/04_(개정)케라스로_Q&A_구현하기_With_SQUAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"Fn7pKyD8dBEq"},"source":["<h1><strong>케라스로 Q&A 구현하기 With SQUAD</strong></h1>\n","<h1><strong><u>Youtube 공무원AI</u></strong></h1>\n","\n","![대체 텍스트](https://i.imgur.com/XSuDmcC.png)\n","\n","이번 튜토리얼에서는 케라스와 BERT를 활용하여 <strong>SQUAD(Standford Question and Answering Dataset)</strong>을 실습해보고자 합니다.\n","\n","SQUAD는 문장과 질문(Question)을 입력하면 그에 해당하는 답(ANSWER)를 알려주는 문제입니다.  \n","즉 AI가 영어 독해 문제를 풀어주는 것입니다.   \n","Tensorflow나 PyTorch로 SQUAD를 구현하는 코드들은 인터넷에 많지만 초보자 입장에서는 코드를 봐도 구현하기가 상당히 어렵습니다. \n","막상 코드를 돌려봐도 어떤 원리로 돌아가는지 알기 어렵습니다.  \n"," \n","그래서 KERAS를 활용하여 쉽게 SQUAD를 구현해보고자 합니다.  \n","본 튜토리얼은 1)SQUAD 이해 2) BERT INPUT 만들기 3) SQUAD 구현 4) SQUAD 예측 총 4단계로 구성되어 있습니다.  \n","각 단계마다 이해하기 쉬운 설명을 곁들이도록 하겠습니다. \n"]},{"cell_type":"markdown","metadata":{"id":"_LeBONl3Rt6J"},"source":["<h2><strong> 1. SQUAD 개념 이해</strong></h2>  \n","SQUAD 문제는 아래와 같습니다.\n","Question을 하면 AI가 Context를 읽고 Answer를 출력값으로 알려주게 됩니다.  \n","처음에 나온 ANSWER는  AI가 예측한 정답입니다. 그리고 원래 정답은 Von Miller인데 왜 von miller ##r로 예측이 되었냐면, 실제로 예측한 것은 bert에 들어간 sentencepiece 토큰이 예측 된 것이기 때문입니다.  \n","  \n","\n","![Imgur](https://i.imgur.com/sjf7vyj.png)  \n","\n","사실 SQUAD는 ANSWER를 다 예측하는 것이 아니라, ANSWER 중에서도 시작단어와 끝 단어만을 예측합니다. 시작과 끝을 알면 자연스럽게 가운데 위치한 글자들도 예측이 되는 것이겠지요. 그리고 SQUAD 문제를 풀기 위해서 BERT 알고리즘을 사용합니다.  \n","  \n","\n","위 그림에서 SQUAD는 ANSWER를 다 예측하는 것이 아니라, ANSWER 중에서도 시작단어와 끝 단어만을 예측합니다. 시작과 끝을 알면 자연스럽게 가운데 위치한 글자들도 예측이 되는 것이겠지요. 그리고 SQUAD 문제를 풀기 위해서 BERT 알고리즘을 사용합니다.  \n","  \n","  ![imgur](https://i.imgur.com/xhbWa13.png)  \n","  위 그림에서 버트 모형에 들어가는 인풋은 Question(질문)과 Paragraph(지문)입니다.  \n","  QUESTION과 PARAGRAPH가 버트의 인풋으로 들어가면 아웃풋 값으로 처음 단어와 큰 단어를 알려주게 됩니다.  \n","\n","  Train Data의 생김새입니다. 총 87599개의 문장이 있습니다. Question과 Context를 인풋으로 받아서, 아웃풋으로 Text(정답)을 맞추게 됩니다.\n","  ![Imgur](https://i.imgur.com/rzTXJ1W.png)\n","\n","  Test Data의 생김새입니다. 총 10,570개의 문장으로 이루어져 있으며, question과 context를 인풋으로 넣으면, 파인튜닝된 버트 모형이 text를 아웃풋 값으로 출력하게 됩니다.\n","  ![Imgur](https://i.imgur.com/s9zqAvv.png)\n"]},{"cell_type":"markdown","metadata":{"id":"TwFvMz0AwGJ-"},"source":["<h2><strong>2. BERT 사전학습 모형 가져오기</strong></h2>\n","\n","https://github.com/google-research/bert 에 접속하셔서 BERT-Base, Multilingual Cased: 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters 파일을 다운 받으시길 바랍니다.\n","\n","![대체 텍스트](https://i.imgur.com/RlutYyW.png)\n","\n","<p>사전 학습 모형을 다운 받으셨다면, Colab에서 활용하기 위해 구글 GDRIVE에 업로드 하시길 바랍니다.</p>\n","\n","![대체 텍스트](https://i.imgur.com/uM97zAQ.png)\n","\n","\n","\n","<p> 누구나 쉽게 실습할 수 있도록 구글 Colaboratory를 활용하였습니다<br> <u>런타임->런타임 유형 변경에서 GPU를 꼭 선택하시기 바랍니다.</u><br>\n","그리고 데이터는 구글 G드라이브에 넣어 두었습니다. 사정에 맞게 폴더 경로를 변경하시기 바랍니다.</p>\n"]},{"cell_type":"code","metadata":{"id":"SwK1tYV_uayz","executionInfo":{"status":"ok","timestamp":1601723065312,"user_tz":-540,"elapsed":13204,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"b15ce11b-023b-4405-8c13-87da0e1c3d1d","colab":{"base_uri":"https://localhost:8080/","height":209}},"source":["# wget을 활용해서 bert 모델 다운로드 가능\n","!wget https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-10-03 11:04:12--  https://storage.googleapis.com/bert_models/2018_11_23/multi_cased_L-12_H-768_A-12.zip\n","Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.97.128, 74.125.203.128, 64.233.188.128, ...\n","Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.97.128|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 662903077 (632M) [application/zip]\n","Saving to: ‘multi_cased_L-12_H-768_A-12.zip’\n","\n","multi_cased_L-12_H- 100%[===================>] 632.19M  63.6MB/s    in 11s     \n","\n","2020-10-03 11:04:25 (56.2 MB/s) - ‘multi_cased_L-12_H-768_A-12.zip’ saved [662903077/662903077]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sx2147Ndq3lS"},"source":["import os\n","if \"bert\" not in os.listdir():\n","  os.makedirs(\"bert\")\n","else:\n","  pass\n","\n","import zipfile\n","import shutil\n","         \n","bert_zip = zipfile.ZipFile('multi_cased_L-12_H-768_A-12.zip')\n","bert_zip.extractall('bert')\n"," \n","bert_zip.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3yaskz4Aujj1"},"source":["def copytree(src, dst, symlinks=False, ignore=None):\n","    for item in os.listdir(src):\n","        s = os.path.join(src, item)\n","        d = os.path.join(dst, item)\n","        if os.path.isdir(s):\n","            shutil.copytree(s, d, symlinks, ignore)\n","        else:\n","            shutil.copy2(s, d)\n","\n","copytree(\"bert/multi_cased_L-12_H-768_A-12\", \"bert\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4a7LicBl40e","executionInfo":{"status":"ok","timestamp":1601723117450,"user_tz":-540,"elapsed":5663,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"e8c57424-1561-4139-ec9d-b358c4e5ab84","colab":{"base_uri":"https://localhost:8080/","height":401}},"source":["!wget https://raw.githubusercontent.com/nate-parrott/squad/master/data/train-v1.1.json\n","!wget https://raw.githubusercontent.com/nate-parrott/squad/master/data/dev-v1.1.json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-10-03 11:05:12--  https://raw.githubusercontent.com/nate-parrott/squad/master/data/train-v1.1.json\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 30288272 (29M) [text/plain]\n","Saving to: ‘train-v1.1.json’\n","\n","train-v1.1.json     100%[===================>]  28.88M  45.4MB/s    in 0.6s    \n","\n","2020-10-03 11:05:15 (45.4 MB/s) - ‘train-v1.1.json’ saved [30288272/30288272]\n","\n","--2020-10-03 11:05:15--  https://raw.githubusercontent.com/nate-parrott/squad/master/data/dev-v1.1.json\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4854279 (4.6M) [text/plain]\n","Saving to: ‘dev-v1.1.json’\n","\n","dev-v1.1.json       100%[===================>]   4.63M  18.2MB/s    in 0.3s    \n","\n","2020-10-03 11:05:16 (18.2 MB/s) - ‘dev-v1.1.json’ saved [4854279/4854279]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zXWE_XTqmta6","executionInfo":{"status":"ok","timestamp":1601723121732,"user_tz":-540,"elapsed":1053,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"aeb568f1-b7ef-4c76-d716-5ab2144439e3","colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["os.listdir()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['.config',\n"," 'dev-v1.1.json',\n"," 'train-v1.1.json',\n"," 'bert',\n"," 'multi_cased_L-12_H-768_A-12.zip',\n"," 'sample_data']"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"_Tkah2Z4n_z6"},"source":["\n","##**구글 드라이브와 Colab을 연동합니다**"]},{"cell_type":"code","metadata":{"id":"hPs7cH6On_Qa","executionInfo":{"status":"ok","timestamp":1601723154597,"user_tz":-540,"elapsed":23698,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"7a2745a2-28de-45dc-e2ea-8d2c984b3574","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HQ6glcvV1h73"},"source":["<h2><strong>본격적으로 케라스와 버트를 활용하여 SQUAD 예측 모델을 만들어 보겠습니다.</strong></h2>"]},{"cell_type":"markdown","metadata":{"id":"QVGCHkKN0TZ3"},"source":["텐서플로우, 판다스, 넘파이, 케라스 등 필요한 모듈들을 임포트합니다"]},{"cell_type":"code","metadata":{"id":"Ffa---Ov8WfF","executionInfo":{"status":"ok","timestamp":1601723183522,"user_tz":-540,"elapsed":6045,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"ebd8ac0e-08b2-4631-85bf-3dd9d7c81e2e","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["%tensorflow_version 1.x\n","import tensorflow as tf\n","\n","import pandas as pd\n","import numpy as np  \n","import re\n","import pickle\n","\n","import keras as keras\n","from keras.models import load_model\n","from keras import backend as K\n","from keras import Input, Model\n","from keras import optimizers\n","\n","from keras import backend as K\n","from keras.layers import Layer\n","\n","import codecs\n","from tqdm import tqdm\n","import shutil\n","import json"],"execution_count":null,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"},{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"bniU2r9PouUO"},"source":["import warnings\n","import tensorflow as tf\n","warnings.filterwarnings(action='ignore')\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n","tf.logging.set_verbosity(tf.logging.ERROR)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A3bWhmOP0mOv"},"source":["케라스에서 Bert 활용을 쉽게 만들어주는 모듈 keras-bert를 설치합니다<br>그리고 Adam optimizer의 수정판인 keras-radam 모듈을 임포트합니다."]},{"cell_type":"code","metadata":{"id":"09QnmSZXGbbD","executionInfo":{"status":"ok","timestamp":1601723211629,"user_tz":-540,"elapsed":19503,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"f5fe74e9-16af-4a4d-9554-b443e3ccc107","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install keras-bert\n","!pip install keras-radam"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting keras-bert\n","  Downloading https://files.pythonhosted.org/packages/e2/7f/95fabd29f4502924fa3f09ff6538c5a7d290dfef2c2fe076d3d1a16e08f0/keras-bert-0.86.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-bert) (1.18.5)\n","Collecting Keras>=2.4.3\n","  Downloading https://files.pythonhosted.org/packages/44/e1/dc0757b20b56c980b5553c1b5c4c32d378c7055ab7bfa92006801ad359ab/Keras-2.4.3-py2.py3-none-any.whl\n","Collecting keras-transformer>=0.38.0\n","  Downloading https://files.pythonhosted.org/packages/89/6c/d6f0c164f4cc16fbc0d0fea85f5526e87a7d2df7b077809e422a7e626150/keras-transformer-0.38.0.tar.gz\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.4.3->keras-bert) (2.10.0)\n","Collecting keras-pos-embd>=0.11.0\n","  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n","Collecting keras-multi-head>=0.27.0\n","  Downloading https://files.pythonhosted.org/packages/e6/32/45adf2549450aca7867deccfa04af80a0ab1ca139af44b16bc669e0e09cd/keras-multi-head-0.27.0.tar.gz\n","Collecting keras-layer-normalization>=0.14.0\n","  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n","Collecting keras-position-wise-feed-forward>=0.6.0\n","  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n","Collecting keras-embed-sim>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/57/ef/61a1e39082c9e1834a2d09261d4a0b69f7c818b359216d4e1912b20b1c86/keras-embed-sim-0.8.0.tar.gz\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras>=2.4.3->keras-bert) (1.15.0)\n","Collecting keras-self-attention==0.46.0\n","  Downloading https://files.pythonhosted.org/packages/15/6b/c804924a056955fa1f3ff767945187103cfc851ba9bd0fc5a6c6bc18e2eb/keras-self-attention-0.46.0.tar.gz\n","Building wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n","  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-bert: filename=keras_bert-0.86.0-cp36-none-any.whl size=34145 sha256=7b387056b91cf7020d00830ea61d03980e234fdb5ba25d59fa36b381cb5275ec\n","  Stored in directory: /root/.cache/pip/wheels/66/f0/b1/748128b58562fc9e31b907bb5e2ab6a35eb37695e83911236b\n","  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-transformer: filename=keras_transformer-0.38.0-cp36-none-any.whl size=12942 sha256=2886c83763d7627fa112c750a11595fa748270753d81f339708477767bdd2290\n","  Stored in directory: /root/.cache/pip/wheels/e5/fb/3a/37b2b9326c799aa010ae46a04ddb04f320d8c77c0b7e837f4e\n","  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7554 sha256=6ea373da23d465e505022aa4d3df0111fad6f80bf72c13af985db8af18ac6607\n","  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n","  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-multi-head: filename=keras_multi_head-0.27.0-cp36-none-any.whl size=15612 sha256=d5814391268f046f81804012da7010fdf26789e0b3998568eb21694bbd8ac111\n","  Stored in directory: /root/.cache/pip/wheels/b5/b4/49/0a0c27dcb93c13af02fea254ff51d1a43a924dd4e5b7a7164d\n","  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=f859a81444e8f383522d021eebf2df9fa934d04b1e7cdb70a9dbd74078c45ae8\n","  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n","  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5626 sha256=5f0ffa4754d4a7b1b86841af76110c7101391e87d05f62781cbb3dbd5be90b43\n","  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n","  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.8.0-cp36-none-any.whl size=4559 sha256=74631ce56e82a8687bf5d5024781e5853432e7c67f88ead133998619e420cb99\n","  Stored in directory: /root/.cache/pip/wheels/49/45/8b/c111f6cc8bec253e984677de73a6f4f5d2f1649f42aac191c8\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.46.0-cp36-none-any.whl size=17278 sha256=1bf152d03d6bd2b2b387f942f8a81034a455b318f8f32171db6974b6f6ea0453\n","  Stored in directory: /root/.cache/pip/wheels/d2/2e/80/fec4c05eb23c8e13b790e26d207d6e0ffe8013fad8c6bdd4d2\n","Successfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n","Installing collected packages: Keras, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\n","  Found existing installation: Keras 2.3.1\n","    Uninstalling Keras-2.3.1:\n","      Successfully uninstalled Keras-2.3.1\n","Successfully installed Keras-2.4.3 keras-bert-0.86.0 keras-embed-sim-0.8.0 keras-layer-normalization-0.14.0 keras-multi-head-0.27.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.46.0 keras-transformer-0.38.0\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["keras"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["Collecting keras-radam\n","  Downloading https://files.pythonhosted.org/packages/46/8d/b83ccaa94253fbc920b21981f038393041d92236bb541751b98a66a2ac1d/keras-radam-0.15.0.tar.gz\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-radam) (1.18.5)\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-radam) (2.4.3)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-radam) (1.4.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-radam) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-radam) (3.13)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras-radam) (1.15.0)\n","Building wheels for collected packages: keras-radam\n","  Building wheel for keras-radam (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-radam: filename=keras_radam-0.15.0-cp36-none-any.whl size=14686 sha256=4f37c6e9a0f3dcab04af57197b899dd65bb4d0add13ceed2d9062845ef62d6fb\n","  Stored in directory: /root/.cache/pip/wheels/79/a0/c0/670b0a118e8f078539fafec7bd02eba0af921f745660c7f83f\n","Successfully built keras-radam\n","Installing collected packages: keras-radam\n","Successfully installed keras-radam-0.15.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cyIoeOBH04lW"},"source":["keras-bert 라이브러리에서 버트 모형 활용에 필요한 모듈들을 임포트합니다"]},{"cell_type":"code","metadata":{"id":"FZv-5ALnGdwI"},"source":["from keras_bert import load_trained_model_from_checkpoint, load_vocabulary\n","from keras_bert import Tokenizer\n","from keras_bert import AdamWarmup, calc_train_steps\n","\n","from keras_radam import RAdam"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kVGK2j5tKJgS"},"source":["SQUAD JSON파일을 PANDAS DATAFRAME으로 만들어주는 함수를 정의합니다.  \n","출처 : https://www.kaggle.com/sanjay11100/squad-stanford-q-a-json-to-pandas-dataframe"]},{"cell_type":"code","metadata":{"id":"zgWmFtiaIZQK"},"source":["def squad_json_to_dataframe_train(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n","                           verbose = 1):\n","    \"\"\"\n","    input_file_path: path to the squad json file.\n","    record_path: path to deepest level in json file default value is\n","    ['data','paragraphs','qas','answers']\n","    verbose: 0 to suppress it default is 1\n","    \"\"\"\n","    if verbose:\n","        print(\"Reading the json file\")    \n","    file = json.loads(open(input_file_path).read())\n","    if verbose:\n","        print(\"processing...\")\n","    # parsing different level's in the json file\n","    js = pd.io.json.json_normalize(file , record_path )\n","    m = pd.io.json.json_normalize(file, record_path[:-1] )\n","    r = pd.io.json.json_normalize(file,record_path[:-2])\n","    \n","    #combining it into single dataframe\n","    idx = np.repeat(r['context'].values, r.qas.str.len())\n","    ndx  = np.repeat(m['id'].values,m['answers'].str.len())\n","    m['context'] = idx\n","    js['q_idx'] = ndx\n","    main = pd.concat([ m[['id','question','context']].set_index('id'),js.set_index('q_idx')],1,sort=False).reset_index()\n","    main['c_id'] = main['context'].factorize()[0]\n","    if verbose:\n","        print(\"shape of the dataframe is {}\".format(main.shape))\n","        print(\"Done\")\n","    return main"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Eoez6gHVGeP8"},"source":["SQUAD 데이터를 PANDAS DATAFRAME 형식으로 로드합니다."]},{"cell_type":"code","metadata":{"id":"9j2vCEzCMW4K","executionInfo":{"status":"ok","timestamp":1601723243248,"user_tz":-540,"elapsed":5509,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"1b145dc0-8fb3-4500-ecd2-245f9bc17a43","colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["train = squad_json_to_dataframe_train(\"train-v1.1.json\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Reading the json file\n","processing...\n","shape of the dataframe is (87599, 6)\n","Done\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"PnnEcb9mQxV1"},"source":["SQUAD 예측을 위한 훈련 데이터가 잘 로드되었습니다.  \n","**question 칼럼이 질문, context 칼럼이 문장으로 인풋으로 들어갑니다.  \n","아웃풋 값(정답)은 text 칼럼에서 시작 단어와 끝 단어 두 개 입니다. 예를 들어서, text 값이 Saint Bernadette Soubirous라면, 정답은 시작 단어인 Saint와 끝 단어인 Soubrious입니다.**  \n","  \n","그리고 SQUAD 문제의 특징은, 정답에 해당하는 아웃풋 값(text)이 context 안에 있다는 것입니다. 참고로 answer_start는 무시하셔도 됩니다. 왜냐하면 answer_start는 context 내에서 단어를 쪼갠 다음 쪼갠 것을 하나 하나 세어서 몇번째에 정답이 위치하는지를 알려주는 것입니다. 예를 들자면, context를 abcdefg라고 가정했을시 e가 정답(text)이라면, answer_start는 5가 됩니다. \n","본 SQAUD 문제에서는 단어를 쪼갠 것을 하나 하나의 위치를 예측하는 것이 아니라, 단어의 시작 위치와 끝 위치를 예측하는 것이기 때문에 answer_start를 무시하셔도 됩니다."]},{"cell_type":"code","metadata":{"id":"waSYuY0kMfW5","executionInfo":{"status":"ok","timestamp":1601723248701,"user_tz":-540,"elapsed":1054,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"ac040f63-9faf-40fe-b20a-4bacdce2603b","colab":{"base_uri":"https://localhost:8080/","height":572}},"source":["train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>question</th>\n","      <th>context</th>\n","      <th>answer_start</th>\n","      <th>text</th>\n","      <th>c_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>5733be284776f41900661182</td>\n","      <td>To whom did the Virgin Mary allegedly appear i...</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>515</td>\n","      <td>Saint Bernadette Soubirous</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>5733be284776f4190066117f</td>\n","      <td>What is in front of the Notre Dame Main Building?</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>188</td>\n","      <td>a copper statue of Christ</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>5733be284776f41900661180</td>\n","      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>279</td>\n","      <td>the Main Building</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>5733be284776f41900661181</td>\n","      <td>What is the Grotto at Notre Dame?</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>381</td>\n","      <td>a Marian place of prayer and reflection</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5733be284776f4190066117e</td>\n","      <td>What sits on top of the Main Building at Notre...</td>\n","      <td>Architecturally, the school has a Catholic cha...</td>\n","      <td>92</td>\n","      <td>a golden statue of the Virgin Mary</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>87594</th>\n","      <td>5735d259012e2f140011a09d</td>\n","      <td>In what US state did Kathmandu first establish...</td>\n","      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n","      <td>229</td>\n","      <td>Oregon</td>\n","      <td>18890</td>\n","    </tr>\n","    <tr>\n","      <th>87595</th>\n","      <td>5735d259012e2f140011a09e</td>\n","      <td>What was Yangon previously known as?</td>\n","      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n","      <td>414</td>\n","      <td>Rangoon</td>\n","      <td>18890</td>\n","    </tr>\n","    <tr>\n","      <th>87596</th>\n","      <td>5735d259012e2f140011a09f</td>\n","      <td>With what Belorussian city does Kathmandu have...</td>\n","      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n","      <td>476</td>\n","      <td>Minsk</td>\n","      <td>18890</td>\n","    </tr>\n","    <tr>\n","      <th>87597</th>\n","      <td>5735d259012e2f140011a0a0</td>\n","      <td>In what year did Kathmandu create its initial ...</td>\n","      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n","      <td>199</td>\n","      <td>1975</td>\n","      <td>18890</td>\n","    </tr>\n","    <tr>\n","      <th>87598</th>\n","      <td>5735d259012e2f140011a0a1</td>\n","      <td>What is KMC an initialism of?</td>\n","      <td>Kathmandu Metropolitan City (KMC), in order to...</td>\n","      <td>0</td>\n","      <td>Kathmandu Metropolitan City</td>\n","      <td>18890</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>87599 rows × 6 columns</p>\n","</div>"],"text/plain":["                          index  ...   c_id\n","0      5733be284776f41900661182  ...      0\n","1      5733be284776f4190066117f  ...      0\n","2      5733be284776f41900661180  ...      0\n","3      5733be284776f41900661181  ...      0\n","4      5733be284776f4190066117e  ...      0\n","...                         ...  ...    ...\n","87594  5735d259012e2f140011a09d  ...  18890\n","87595  5735d259012e2f140011a09e  ...  18890\n","87596  5735d259012e2f140011a09f  ...  18890\n","87597  5735d259012e2f140011a0a0  ...  18890\n","87598  5735d259012e2f140011a0a1  ...  18890\n","\n","[87599 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"2-NebUQmG1w-"},"source":["\n","- bert 훈련을 위한 사전 설정을 합니다. SEQ_LEN은 문장의 최대 길이입니다. SEQ_LEN 보다 문장의 길이가 작다면 남은 부분은 0이 채워지고, 만약에 SEQ_LEN보다 문장 길이가 길다면 SEQ_LEN을 초과하는 부분이 잘리게 됩니다.  \n","본 문제에서는 메모리 문제 등으로 384로 정했습니다.\n","- BATCH_SIZE는 메모리 초과 같은 문제를 방지하기 위해 작은 수인 10으로 정했습니다. 그리고 총 훈련 에포크 수는 2로 정했습니다. 학습율(LR;Learning rate)은 3e-5로 작게 정했습니다.\n","- pretrained_path는 bert 사전학습 모형이 있는 폴더를 의미합니다.\n","- 그리고 우리가 분석할 문장이 들어있는 칼럼의 제목인 document와 긍정인지 부정인지 알려주는 칼럼을 label로 정해줍니다\n"]},{"cell_type":"code","metadata":{"id":"tLYWgZJAtsku"},"source":["SEQ_LEN = 384\n","BATCH_SIZE = 10\n","EPOCHS=2\n","LR=3e-5\n","\n","pretrained_path =\"bert\"\n","config_path = os.path.join(pretrained_path, 'bert_config.json')\n","checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')\n","vocab_path = os.path.join(pretrained_path, 'vocab.txt')\n","\n","DATA_COLUMN = \"context\"\n","QUESTION_COLUMN = \"question\"\n","TEXT = \"text\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IaU-13a9H4YJ"},"source":["vocab.txt에 있는 단어에 인덱스를 추가해주는 token_dict라는 딕셔너리를 생성합니다.  \n","우리가 분석할 문장이 토큰화가 되고, 그 다음에는 인덱스(숫자)로 변경되어서 버트 신경망에 인풋으로 들어게 됩니다."]},{"cell_type":"code","metadata":{"id":"-ph2YQXg2iz5"},"source":["token_dict = {}\n","with codecs.open(vocab_path, 'r', 'utf8') as reader:\n","    for line in reader:\n","        token = line.strip()\n","        if \"_\" in token:\n","          token = token.replace(\"_\",\"\")\n","          token = \"##\" + token\n","        token_dict[token] = len(token_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0WhiwaBCIXqv"},"source":["- BERT의 토큰화는 단어를 분리하는 토큰화 방식입니다. wordpiece(단어조각?) 방식이라고 하는데, 이는 한국어를 형태소로 꼭 변환해야 할 문제를 해결해주며, 의미가 있는 단어는 밀접하게 연관이 되게 하는 장점까지 갖추고 있습니다.\n","- 단어의 첫 시작은 ##가 붙지 않지만, 단어에 포함되면서 단어의 시작이 아닌 부분에는 ##가 붙는 것이 특징입니다."]},{"cell_type":"code","metadata":{"id":"88T_cMA4tsk2"},"source":["tokenizer = Tokenizer(token_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W0dmxrOhJ9vD"},"source":["토큰화가 잘 되었는지 확인해 봅니다.\n","버트 모형은 문장 앞에 꼭 [CLS]라는 문자가 위치하고, [SEP]라는 문자가 끝에 위치합니다.  \n","[CLS]는 문장의 시작, [SEP]는 문장의 끝을 의미합니다."]},{"cell_type":"code","metadata":{"id":"-4EWrSTCMkzk","executionInfo":{"status":"ok","timestamp":1601723264767,"user_tz":-540,"elapsed":1022,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"38d03076-d9d7-4dbb-bc40-6daa403e0a92","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(tokenizer.tokenize(\"keras is reall fun.\"), tokenizer.tokenize(\"we can manipulate AI.\"))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['[CLS]', 'keras', 'is', 'real', '##l', 'fun', '.', '[SEP]'] ['[CLS]', 'we', 'can', 'mani', '##pul', '##ate', 'ai', '.', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kOuqggRsORwN","executionInfo":{"status":"ok","timestamp":1601723266946,"user_tz":-540,"elapsed":1054,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"73d16500-b247-47be-d402-bb4c77ceffd1","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["token_dict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'[PAD]': 0,\n"," '[unused1]': 1,\n"," '[unused2]': 2,\n"," '[unused3]': 3,\n"," '[unused4]': 4,\n"," '[unused5]': 5,\n"," '[unused6]': 6,\n"," '[unused7]': 7,\n"," '[unused8]': 8,\n"," '[unused9]': 9,\n"," '[unused10]': 10,\n"," '[unused11]': 11,\n"," '[unused12]': 12,\n"," '[unused13]': 13,\n"," '[unused14]': 14,\n"," '[unused15]': 15,\n"," '[unused16]': 16,\n"," '[unused17]': 17,\n"," '[unused18]': 18,\n"," '[unused19]': 19,\n"," '[unused20]': 20,\n"," '[unused21]': 21,\n"," '[unused22]': 22,\n"," '[unused23]': 23,\n"," '[unused24]': 24,\n"," '[unused25]': 25,\n"," '[unused26]': 26,\n"," '[unused27]': 27,\n"," '[unused28]': 28,\n"," '[unused29]': 29,\n"," '[unused30]': 30,\n"," '[unused31]': 31,\n"," '[unused32]': 32,\n"," '[unused33]': 33,\n"," '[unused34]': 34,\n"," '[unused35]': 35,\n"," '[unused36]': 36,\n"," '[unused37]': 37,\n"," '[unused38]': 38,\n"," '[unused39]': 39,\n"," '[unused40]': 40,\n"," '[unused41]': 41,\n"," '[unused42]': 42,\n"," '[unused43]': 43,\n"," '[unused44]': 44,\n"," '[unused45]': 45,\n"," '[unused46]': 46,\n"," '[unused47]': 47,\n"," '[unused48]': 48,\n"," '[unused49]': 49,\n"," '[unused50]': 50,\n"," '[unused51]': 51,\n"," '[unused52]': 52,\n"," '[unused53]': 53,\n"," '[unused54]': 54,\n"," '[unused55]': 55,\n"," '[unused56]': 56,\n"," '[unused57]': 57,\n"," '[unused58]': 58,\n"," '[unused59]': 59,\n"," '[unused60]': 60,\n"," '[unused61]': 61,\n"," '[unused62]': 62,\n"," '[unused63]': 63,\n"," '[unused64]': 64,\n"," '[unused65]': 65,\n"," '[unused66]': 66,\n"," '[unused67]': 67,\n"," '[unused68]': 68,\n"," '[unused69]': 69,\n"," '[unused70]': 70,\n"," '[unused71]': 71,\n"," '[unused72]': 72,\n"," '[unused73]': 73,\n"," '[unused74]': 74,\n"," '[unused75]': 75,\n"," '[unused76]': 76,\n"," '[unused77]': 77,\n"," '[unused78]': 78,\n"," '[unused79]': 79,\n"," '[unused80]': 80,\n"," '[unused81]': 81,\n"," '[unused82]': 82,\n"," '[unused83]': 83,\n"," '[unused84]': 84,\n"," '[unused85]': 85,\n"," '[unused86]': 86,\n"," '[unused87]': 87,\n"," '[unused88]': 88,\n"," '[unused89]': 89,\n"," '[unused90]': 90,\n"," '[unused91]': 91,\n"," '[unused92]': 92,\n"," '[unused93]': 93,\n"," '[unused94]': 94,\n"," '[unused95]': 95,\n"," '[unused96]': 96,\n"," '[unused97]': 97,\n"," '[unused98]': 98,\n"," '[unused99]': 99,\n"," '[UNK]': 100,\n"," '[CLS]': 101,\n"," '[SEP]': 102,\n"," '[MASK]': 103,\n"," '<S>': 104,\n"," '<T>': 105,\n"," '!': 106,\n"," '\"': 107,\n"," '#': 108,\n"," '$': 109,\n"," '%': 110,\n"," '&': 111,\n"," \"'\": 112,\n"," '(': 113,\n"," ')': 114,\n"," '*': 115,\n"," '+': 116,\n"," ',': 117,\n"," '-': 118,\n"," '.': 119,\n"," '/': 120,\n"," '0': 121,\n"," '1': 122,\n"," '2': 123,\n"," '3': 124,\n"," '4': 125,\n"," '5': 126,\n"," '6': 127,\n"," '7': 128,\n"," '8': 129,\n"," '9': 130,\n"," ':': 131,\n"," ';': 132,\n"," '<': 133,\n"," '=': 134,\n"," '>': 135,\n"," '?': 136,\n"," '@': 137,\n"," 'A': 138,\n"," 'B': 139,\n"," 'C': 140,\n"," 'D': 141,\n"," 'E': 142,\n"," 'F': 143,\n"," 'G': 144,\n"," 'H': 145,\n"," 'I': 146,\n"," 'J': 147,\n"," 'K': 148,\n"," 'L': 149,\n"," 'M': 150,\n"," 'N': 151,\n"," 'O': 152,\n"," 'P': 153,\n"," 'Q': 154,\n"," 'R': 155,\n"," 'S': 156,\n"," 'T': 157,\n"," 'U': 158,\n"," 'V': 159,\n"," 'W': 160,\n"," 'X': 161,\n"," 'Y': 162,\n"," 'Z': 163,\n"," '[': 164,\n"," '\\\\': 165,\n"," ']': 166,\n"," '^': 167,\n"," '##': 168,\n"," 'a': 169,\n"," 'b': 170,\n"," 'c': 171,\n"," 'd': 172,\n"," 'e': 173,\n"," 'f': 174,\n"," 'g': 175,\n"," 'h': 176,\n"," 'i': 177,\n"," 'j': 178,\n"," 'k': 179,\n"," 'l': 180,\n"," 'm': 181,\n"," 'n': 182,\n"," 'o': 183,\n"," 'p': 184,\n"," 'q': 185,\n"," 'r': 186,\n"," 's': 187,\n"," 't': 188,\n"," 'u': 189,\n"," 'v': 190,\n"," 'w': 191,\n"," 'x': 192,\n"," 'y': 193,\n"," 'z': 194,\n"," '{': 195,\n"," '|': 196,\n"," '}': 197,\n"," '~': 198,\n"," '¡': 199,\n"," '¢': 200,\n"," '£': 201,\n"," '¥': 202,\n"," '¦': 203,\n"," '§': 204,\n"," '¨': 205,\n"," '©': 206,\n"," 'ª': 207,\n"," '«': 208,\n"," '¬': 209,\n"," '®': 210,\n"," '°': 211,\n"," '±': 212,\n"," '²': 213,\n"," '³': 214,\n"," 'µ': 215,\n"," '¶': 216,\n"," '·': 217,\n"," '¹': 218,\n"," 'º': 219,\n"," '»': 220,\n"," '¼': 221,\n"," '½': 222,\n"," '¾': 223,\n"," '¿': 224,\n"," 'À': 225,\n"," 'Á': 226,\n"," 'Â': 227,\n"," 'Ã': 228,\n"," 'Ä': 229,\n"," 'Å': 230,\n"," 'Æ': 231,\n"," 'Ç': 232,\n"," 'È': 233,\n"," 'É': 234,\n"," 'Ê': 235,\n"," 'Ë': 236,\n"," 'Ì': 237,\n"," 'Í': 238,\n"," 'Î': 239,\n"," 'Ð': 240,\n"," 'Ñ': 241,\n"," 'Ò': 242,\n"," 'Ó': 243,\n"," 'Ô': 244,\n"," 'Õ': 245,\n"," 'Ö': 246,\n"," '×': 247,\n"," 'Ø': 248,\n"," 'Ú': 249,\n"," 'Ü': 250,\n"," 'Ý': 251,\n"," 'Þ': 252,\n"," 'ß': 253,\n"," 'à': 254,\n"," 'á': 255,\n"," 'â': 256,\n"," 'ã': 257,\n"," 'ä': 258,\n"," 'å': 259,\n"," 'æ': 260,\n"," 'ç': 261,\n"," 'è': 262,\n"," 'é': 263,\n"," 'ê': 264,\n"," 'ë': 265,\n"," 'ì': 266,\n"," 'í': 267,\n"," 'î': 268,\n"," 'ï': 269,\n"," 'ð': 270,\n"," 'ñ': 271,\n"," 'ò': 272,\n"," 'ó': 273,\n"," 'ô': 274,\n"," 'õ': 275,\n"," 'ö': 276,\n"," '÷': 277,\n"," 'ø': 278,\n"," 'ù': 279,\n"," 'ú': 280,\n"," 'û': 281,\n"," 'ü': 282,\n"," 'ý': 283,\n"," 'þ': 284,\n"," 'ÿ': 285,\n"," 'Ā': 286,\n"," 'ā': 287,\n"," 'Ă': 288,\n"," 'ă': 289,\n"," 'Ą': 290,\n"," 'ą': 291,\n"," 'Ć': 292,\n"," 'ć': 293,\n"," 'Č': 294,\n"," 'č': 295,\n"," 'Ď': 296,\n"," 'ď': 297,\n"," 'Đ': 298,\n"," 'đ': 299,\n"," 'Ē': 300,\n"," 'ē': 301,\n"," 'Ĕ': 302,\n"," 'ĕ': 303,\n"," 'Ė': 304,\n"," 'ė': 305,\n"," 'ę': 306,\n"," 'ě': 307,\n"," 'Ğ': 308,\n"," 'ğ': 309,\n"," 'ġ': 310,\n"," 'Ģ': 311,\n"," 'ģ': 312,\n"," 'Ħ': 313,\n"," 'ħ': 314,\n"," 'ĩ': 315,\n"," 'Ī': 316,\n"," 'ī': 317,\n"," 'Į': 318,\n"," 'į': 319,\n"," 'İ': 320,\n"," 'ı': 321,\n"," 'Ķ': 322,\n"," 'ķ': 323,\n"," 'ĺ': 324,\n"," 'Ļ': 325,\n"," 'ļ': 326,\n"," 'Ľ': 327,\n"," 'ľ': 328,\n"," 'Ł': 329,\n"," 'ł': 330,\n"," 'ń': 331,\n"," 'Ņ': 332,\n"," 'ņ': 333,\n"," 'ň': 334,\n"," 'ŉ': 335,\n"," 'ŋ': 336,\n"," 'Ō': 337,\n"," 'ō': 338,\n"," 'ŏ': 339,\n"," 'Ő': 340,\n"," 'ő': 341,\n"," 'Œ': 342,\n"," 'œ': 343,\n"," 'ŕ': 344,\n"," 'Ř': 345,\n"," 'ř': 346,\n"," 'Ś': 347,\n"," 'ś': 348,\n"," 'Ş': 349,\n"," 'ş': 350,\n"," 'Š': 351,\n"," 'š': 352,\n"," 'Ţ': 353,\n"," 'ţ': 354,\n"," 'Ť': 355,\n"," 'ť': 356,\n"," 'ũ': 357,\n"," 'Ū': 358,\n"," 'ū': 359,\n"," 'ŭ': 360,\n"," 'ů': 361,\n"," 'Ű': 362,\n"," 'ű': 363,\n"," 'ų': 364,\n"," 'ŵ': 365,\n"," 'ŷ': 366,\n"," 'Ź': 367,\n"," 'ź': 368,\n"," 'Ż': 369,\n"," 'ż': 370,\n"," 'Ž': 371,\n"," 'ž': 372,\n"," 'Ə': 373,\n"," 'ƒ': 374,\n"," 'ơ': 375,\n"," 'Ư': 376,\n"," 'ư': 377,\n"," 'ǎ': 378,\n"," 'ǐ': 379,\n"," 'ǔ': 380,\n"," 'ǫ': 381,\n"," 'ǹ': 382,\n"," 'Ș': 383,\n"," 'ș': 384,\n"," 'Ț': 385,\n"," 'ț': 386,\n"," 'ɐ': 387,\n"," 'ɑ': 388,\n"," 'ɔ': 389,\n"," 'ɕ': 390,\n"," 'ə': 391,\n"," 'ɛ': 392,\n"," 'ɡ': 393,\n"," 'ɣ': 394,\n"," 'ɨ': 395,\n"," 'ɪ': 396,\n"," 'ɲ': 397,\n"," 'ɾ': 398,\n"," 'ʁ': 399,\n"," 'ʃ': 400,\n"," 'ʊ': 401,\n"," 'ʎ': 402,\n"," 'ʒ': 403,\n"," 'ʔ': 404,\n"," 'ʙ': 405,\n"," 'ʰ': 406,\n"," 'ʲ': 407,\n"," 'ʳ': 408,\n"," 'ʷ': 409,\n"," 'ʸ': 410,\n"," 'ʻ': 411,\n"," 'ʼ': 412,\n"," 'ʾ': 413,\n"," 'ʿ': 414,\n"," 'ˈ': 415,\n"," 'ː': 416,\n"," 'ˡ': 417,\n"," 'ˢ': 418,\n"," '̀': 419,\n"," '́': 420,\n"," '̃': 421,\n"," '̄': 422,\n"," '̍': 423,\n"," '̥': 424,\n"," '̧': 425,\n"," '̲': 426,\n"," '͡': 427,\n"," '΄': 428,\n"," 'Ά': 429,\n"," 'Έ': 430,\n"," 'Ή': 431,\n"," 'Ί': 432,\n"," 'Ό': 433,\n"," 'Ύ': 434,\n"," 'Ώ': 435,\n"," 'ΐ': 436,\n"," 'Α': 437,\n"," 'Β': 438,\n"," 'Γ': 439,\n"," 'Δ': 440,\n"," 'Ε': 441,\n"," 'Ζ': 442,\n"," 'Η': 443,\n"," 'Θ': 444,\n"," 'Ι': 445,\n"," 'Κ': 446,\n"," 'Λ': 447,\n"," 'Μ': 448,\n"," 'Ν': 449,\n"," 'Ξ': 450,\n"," 'Ο': 451,\n"," 'Π': 452,\n"," 'Ρ': 453,\n"," 'Σ': 454,\n"," 'Τ': 455,\n"," 'Υ': 456,\n"," 'Φ': 457,\n"," 'Χ': 458,\n"," 'Ψ': 459,\n"," 'Ω': 460,\n"," 'ά': 461,\n"," 'έ': 462,\n"," 'ή': 463,\n"," 'ί': 464,\n"," 'α': 465,\n"," 'β': 466,\n"," 'γ': 467,\n"," 'δ': 468,\n"," 'ε': 469,\n"," 'ζ': 470,\n"," 'η': 471,\n"," 'θ': 472,\n"," 'ι': 473,\n"," 'κ': 474,\n"," 'λ': 475,\n"," 'μ': 476,\n"," 'ν': 477,\n"," 'ξ': 478,\n"," 'ο': 479,\n"," 'π': 480,\n"," 'ρ': 481,\n"," 'ς': 482,\n"," 'σ': 483,\n"," 'τ': 484,\n"," 'υ': 485,\n"," 'φ': 486,\n"," 'χ': 487,\n"," 'ψ': 488,\n"," 'ω': 489,\n"," 'ϊ': 490,\n"," 'ϋ': 491,\n"," 'ό': 492,\n"," 'ύ': 493,\n"," 'ώ': 494,\n"," 'Ё': 495,\n"," 'Ђ': 496,\n"," 'Ѓ': 497,\n"," 'Є': 498,\n"," 'Ѕ': 499,\n"," 'І': 500,\n"," 'Ї': 501,\n"," 'Ј': 502,\n"," 'Љ': 503,\n"," 'Њ': 504,\n"," 'Ћ': 505,\n"," 'Ќ': 506,\n"," 'Ў': 507,\n"," 'Џ': 508,\n"," 'А': 509,\n"," 'Б': 510,\n"," 'В': 511,\n"," 'Г': 512,\n"," 'Д': 513,\n"," 'Е': 514,\n"," 'Ж': 515,\n"," 'З': 516,\n"," 'И': 517,\n"," 'Й': 518,\n"," 'К': 519,\n"," 'Л': 520,\n"," 'М': 521,\n"," 'Н': 522,\n"," 'О': 523,\n"," 'П': 524,\n"," 'Р': 525,\n"," 'С': 526,\n"," 'Т': 527,\n"," 'У': 528,\n"," 'Ф': 529,\n"," 'Х': 530,\n"," 'Ц': 531,\n"," 'Ч': 532,\n"," 'Ш': 533,\n"," 'Щ': 534,\n"," 'Ъ': 535,\n"," 'Ы': 536,\n"," 'Ь': 537,\n"," 'Э': 538,\n"," 'Ю': 539,\n"," 'Я': 540,\n"," 'а': 541,\n"," 'б': 542,\n"," 'в': 543,\n"," 'г': 544,\n"," 'д': 545,\n"," 'е': 546,\n"," 'ж': 547,\n"," 'з': 548,\n"," 'и': 549,\n"," 'й': 550,\n"," 'к': 551,\n"," 'л': 552,\n"," 'м': 553,\n"," 'н': 554,\n"," 'о': 555,\n"," 'п': 556,\n"," 'р': 557,\n"," 'с': 558,\n"," 'т': 559,\n"," 'у': 560,\n"," 'ф': 561,\n"," 'х': 562,\n"," 'ц': 563,\n"," 'ч': 564,\n"," 'ш': 565,\n"," 'щ': 566,\n"," 'ъ': 567,\n"," 'ы': 568,\n"," 'ь': 569,\n"," 'э': 570,\n"," 'ю': 571,\n"," 'я': 572,\n"," 'ѐ': 573,\n"," 'ё': 574,\n"," 'ђ': 575,\n"," 'ѓ': 576,\n"," 'є': 577,\n"," 'ѕ': 578,\n"," 'і': 579,\n"," 'ї': 580,\n"," 'ј': 581,\n"," 'љ': 582,\n"," 'њ': 583,\n"," 'ћ': 584,\n"," 'ќ': 585,\n"," 'ѝ': 586,\n"," 'ў': 587,\n"," 'џ': 588,\n"," 'ѣ': 589,\n"," 'Ґ': 590,\n"," 'ґ': 591,\n"," 'Ғ': 592,\n"," 'ғ': 593,\n"," 'Җ': 594,\n"," 'җ': 595,\n"," 'Ҙ': 596,\n"," 'ҙ': 597,\n"," 'Қ': 598,\n"," 'қ': 599,\n"," 'Ҡ': 600,\n"," 'ҡ': 601,\n"," 'ң': 602,\n"," 'ҫ': 603,\n"," 'Ү': 604,\n"," 'ү': 605,\n"," 'Ұ': 606,\n"," 'ұ': 607,\n"," 'Ҳ': 608,\n"," 'ҳ': 609,\n"," 'Ҷ': 610,\n"," 'ҷ': 611,\n"," 'Һ': 612,\n"," 'һ': 613,\n"," 'Ӏ': 614,\n"," 'ӑ': 615,\n"," 'ӗ': 616,\n"," 'Ә': 617,\n"," 'ә': 618,\n"," 'ӣ': 619,\n"," 'Ө': 620,\n"," 'ө': 621,\n"," 'Ӯ': 622,\n"," 'ӯ': 623,\n"," 'ӳ': 624,\n"," 'Ա': 625,\n"," 'Բ': 626,\n"," 'Գ': 627,\n"," 'Դ': 628,\n"," 'Ե': 629,\n"," 'Զ': 630,\n"," 'Է': 631,\n"," 'Ը': 632,\n"," 'Թ': 633,\n"," 'Ժ': 634,\n"," 'Ի': 635,\n"," 'Լ': 636,\n"," 'Խ': 637,\n"," 'Ծ': 638,\n"," 'Կ': 639,\n"," 'Հ': 640,\n"," 'Ձ': 641,\n"," 'Ղ': 642,\n"," 'Ճ': 643,\n"," 'Մ': 644,\n"," 'Յ': 645,\n"," 'Ն': 646,\n"," 'Շ': 647,\n"," 'Ո': 648,\n"," 'Չ': 649,\n"," 'Պ': 650,\n"," 'Ջ': 651,\n"," 'Ռ': 652,\n"," 'Ս': 653,\n"," 'Վ': 654,\n"," 'Տ': 655,\n"," 'Ր': 656,\n"," 'Ց': 657,\n"," 'Ւ': 658,\n"," 'Փ': 659,\n"," 'Ք': 660,\n"," 'Օ': 661,\n"," 'Ֆ': 662,\n"," '՚': 663,\n"," '՛': 664,\n"," '՜': 665,\n"," '՝': 666,\n"," '՞': 667,\n"," 'ա': 668,\n"," 'բ': 669,\n"," 'գ': 670,\n"," 'դ': 671,\n"," 'ե': 672,\n"," 'զ': 673,\n"," 'է': 674,\n"," 'ը': 675,\n"," 'թ': 676,\n"," 'ժ': 677,\n"," 'ի': 678,\n"," 'լ': 679,\n"," 'խ': 680,\n"," 'ծ': 681,\n"," 'կ': 682,\n"," 'հ': 683,\n"," 'ձ': 684,\n"," 'ղ': 685,\n"," 'ճ': 686,\n"," 'մ': 687,\n"," 'յ': 688,\n"," 'ն': 689,\n"," 'շ': 690,\n"," 'ո': 691,\n"," 'չ': 692,\n"," 'պ': 693,\n"," 'ջ': 694,\n"," 'ռ': 695,\n"," 'ս': 696,\n"," 'վ': 697,\n"," 'տ': 698,\n"," 'ր': 699,\n"," 'ց': 700,\n"," 'ւ': 701,\n"," 'փ': 702,\n"," 'ք': 703,\n"," 'օ': 704,\n"," 'ֆ': 705,\n"," 'և': 706,\n"," '։': 707,\n"," '֊': 708,\n"," 'ְ': 709,\n"," 'ֱ': 710,\n"," 'ֲ': 711,\n"," 'ִ': 712,\n"," 'ֵ': 713,\n"," 'ֶ': 714,\n"," 'ַ': 715,\n"," 'ָ': 716,\n"," 'ֹ': 717,\n"," 'ּ': 718,\n"," '־': 719,\n"," 'ׁ': 720,\n"," 'ׂ': 721,\n"," '׃': 722,\n"," 'א': 723,\n"," 'ב': 724,\n"," 'ג': 725,\n"," 'ד': 726,\n"," 'ה': 727,\n"," 'ו': 728,\n"," 'ז': 729,\n"," 'ח': 730,\n"," 'ט': 731,\n"," 'י': 732,\n"," 'ך': 733,\n"," 'כ': 734,\n"," 'ל': 735,\n"," 'ם': 736,\n"," 'מ': 737,\n"," 'ן': 738,\n"," 'נ': 739,\n"," 'ס': 740,\n"," 'ע': 741,\n"," 'ף': 742,\n"," 'פ': 743,\n"," 'ץ': 744,\n"," 'צ': 745,\n"," 'ק': 746,\n"," 'ר': 747,\n"," 'ש': 748,\n"," 'ת': 749,\n"," '׳': 750,\n"," '״': 751,\n"," '،': 752,\n"," '؍': 753,\n"," 'ؒ': 754,\n"," '؛': 755,\n"," '؟': 756,\n"," 'ء': 757,\n"," 'آ': 758,\n"," 'أ': 759,\n"," 'ؤ': 760,\n"," 'إ': 761,\n"," 'ئ': 762,\n"," 'ا': 763,\n"," 'ب': 764,\n"," 'ة': 765,\n"," 'ت': 766,\n"," 'ث': 767,\n"," 'ج': 768,\n"," 'ح': 769,\n"," 'خ': 770,\n"," 'د': 771,\n"," 'ذ': 772,\n"," 'ر': 773,\n"," 'ز': 774,\n"," 'س': 775,\n"," 'ش': 776,\n"," 'ص': 777,\n"," 'ض': 778,\n"," 'ط': 779,\n"," 'ظ': 780,\n"," 'ع': 781,\n"," 'غ': 782,\n"," 'ـ': 783,\n"," 'ف': 784,\n"," 'ق': 785,\n"," 'ك': 786,\n"," 'ل': 787,\n"," 'م': 788,\n"," 'ن': 789,\n"," 'ه': 790,\n"," 'و': 791,\n"," 'ى': 792,\n"," 'ي': 793,\n"," 'ً': 794,\n"," 'ٌ': 795,\n"," 'ٍ': 796,\n"," 'َ': 797,\n"," 'ُ': 798,\n"," 'ِ': 799,\n"," 'ّ': 800,\n"," 'ْ': 801,\n"," 'ٔ': 802,\n"," 'ٛ': 803,\n"," '٠': 804,\n"," '١': 805,\n"," '٢': 806,\n"," '٣': 807,\n"," '٤': 808,\n"," '٥': 809,\n"," '٩': 810,\n"," '٪': 811,\n"," '٫': 812,\n"," '٬': 813,\n"," '٭': 814,\n"," 'ٰ': 815,\n"," 'ٹ': 816,\n"," 'پ': 817,\n"," 'چ': 818,\n"," 'ڈ': 819,\n"," 'ڑ': 820,\n"," 'ژ': 821,\n"," 'ڤ': 822,\n"," 'ک': 823,\n"," 'ڭ': 824,\n"," 'گ': 825,\n"," 'ں': 826,\n"," 'ھ': 827,\n"," 'ۀ': 828,\n"," 'ہ': 829,\n"," 'ۂ': 830,\n"," 'ۃ': 831,\n"," 'ۆ': 832,\n"," 'ۇ': 833,\n"," 'ی': 834,\n"," 'ے': 835,\n"," 'ۓ': 836,\n"," '۔': 837,\n"," '۰': 838,\n"," '۱': 839,\n"," '۲': 840,\n"," '۳': 841,\n"," '۴': 842,\n"," '۵': 843,\n"," '۶': 844,\n"," '۷': 845,\n"," '۸': 846,\n"," '۹': 847,\n"," 'ँ': 848,\n"," 'ं': 849,\n"," 'ः': 850,\n"," 'अ': 851,\n"," 'आ': 852,\n"," 'इ': 853,\n"," 'ई': 854,\n"," 'उ': 855,\n"," 'ऊ': 856,\n"," 'ऋ': 857,\n"," 'ऍ': 858,\n"," 'ऎ': 859,\n"," 'ए': 860,\n"," 'ऐ': 861,\n"," 'ऑ': 862,\n"," 'ओ': 863,\n"," 'औ': 864,\n"," 'क': 865,\n"," 'ख': 866,\n"," 'ग': 867,\n"," 'घ': 868,\n"," 'ङ': 869,\n"," 'च': 870,\n"," 'छ': 871,\n"," 'ज': 872,\n"," 'झ': 873,\n"," 'ञ': 874,\n"," 'ट': 875,\n"," 'ठ': 876,\n"," 'ड': 877,\n"," 'ढ': 878,\n"," 'ण': 879,\n"," 'त': 880,\n"," 'थ': 881,\n"," 'द': 882,\n"," 'ध': 883,\n"," 'न': 884,\n"," 'प': 885,\n"," 'फ': 886,\n"," 'ब': 887,\n"," 'भ': 888,\n"," 'म': 889,\n"," 'य': 890,\n"," 'र': 891,\n"," 'ऱ': 892,\n"," 'ल': 893,\n"," 'ळ': 894,\n"," 'व': 895,\n"," 'श': 896,\n"," 'ष': 897,\n"," 'स': 898,\n"," 'ह': 899,\n"," '़': 900,\n"," 'ऽ': 901,\n"," 'ा': 902,\n"," 'ि': 903,\n"," 'ी': 904,\n"," 'ु': 905,\n"," 'ू': 906,\n"," 'ृ': 907,\n"," 'ॄ': 908,\n"," 'ॅ': 909,\n"," 'ॆ': 910,\n"," 'े': 911,\n"," 'ै': 912,\n"," 'ॉ': 913,\n"," 'ो': 914,\n"," 'ौ': 915,\n"," '्': 916,\n"," 'ॐ': 917,\n"," '॓': 918,\n"," 'ॠ': 919,\n"," '।': 920,\n"," '॥': 921,\n"," '०': 922,\n"," '१': 923,\n"," '२': 924,\n"," '३': 925,\n"," '४': 926,\n"," '५': 927,\n"," '६': 928,\n"," '७': 929,\n"," '८': 930,\n"," '९': 931,\n"," '॰': 932,\n"," 'ॲ': 933,\n"," 'ঁ': 934,\n"," 'ং': 935,\n"," 'ঃ': 936,\n"," 'অ': 937,\n"," 'আ': 938,\n"," 'ই': 939,\n"," 'ঈ': 940,\n"," 'উ': 941,\n"," 'ঊ': 942,\n"," 'ঋ': 943,\n"," 'এ': 944,\n"," 'ঐ': 945,\n"," 'ও': 946,\n"," 'ঔ': 947,\n"," 'ক': 948,\n"," 'খ': 949,\n"," 'গ': 950,\n"," 'ঘ': 951,\n"," 'ঙ': 952,\n"," 'চ': 953,\n"," 'ছ': 954,\n"," 'জ': 955,\n"," 'ঝ': 956,\n"," 'ঞ': 957,\n"," 'ট': 958,\n"," 'ঠ': 959,\n"," 'ড': 960,\n"," 'ঢ': 961,\n"," 'ণ': 962,\n"," 'ত': 963,\n"," 'থ': 964,\n"," 'দ': 965,\n"," 'ধ': 966,\n"," 'ন': 967,\n"," 'প': 968,\n"," 'ফ': 969,\n"," 'ব': 970,\n"," 'ভ': 971,\n"," 'ম': 972,\n"," 'য': 973,\n"," 'র': 974,\n"," 'ল': 975,\n"," 'শ': 976,\n"," 'ষ': 977,\n"," 'স': 978,\n"," 'হ': 979,\n"," '়': 980,\n"," 'া': 981,\n"," 'ি': 982,\n"," 'ী': 983,\n"," 'ু': 984,\n"," 'ূ': 985,\n"," 'ৃ': 986,\n"," 'ে': 987,\n"," 'ৈ': 988,\n"," 'ো': 989,\n"," 'ৌ': 990,\n"," '্': 991,\n"," 'ৎ': 992,\n"," '০': 993,\n"," '১': 994,\n"," '২': 995,\n"," '৩': 996,\n"," '৪': 997,\n"," '৫': 998,\n"," '৬': 999,\n"," ...}"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"rCC_Z1TNSODT"},"source":["token dict의 key 값과 value 값을 바꾼 reverse_token_dict를 정의합니다."]},{"cell_type":"code","metadata":{"id":"oF_7BhoWw6_E"},"source":["reverse_token_dict = {v : k for k, v in token_dict.items()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jeYV0qlaxepy","executionInfo":{"status":"ok","timestamp":1601723272194,"user_tz":-540,"elapsed":825,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"11e835a8-4212-41c8-e061-3ec17b2c932a","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["reverse_token_dict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{0: '[PAD]',\n"," 1: '[unused1]',\n"," 2: '[unused2]',\n"," 3: '[unused3]',\n"," 4: '[unused4]',\n"," 5: '[unused5]',\n"," 6: '[unused6]',\n"," 7: '[unused7]',\n"," 8: '[unused8]',\n"," 9: '[unused9]',\n"," 10: '[unused10]',\n"," 11: '[unused11]',\n"," 12: '[unused12]',\n"," 13: '[unused13]',\n"," 14: '[unused14]',\n"," 15: '[unused15]',\n"," 16: '[unused16]',\n"," 17: '[unused17]',\n"," 18: '[unused18]',\n"," 19: '[unused19]',\n"," 20: '[unused20]',\n"," 21: '[unused21]',\n"," 22: '[unused22]',\n"," 23: '[unused23]',\n"," 24: '[unused24]',\n"," 25: '[unused25]',\n"," 26: '[unused26]',\n"," 27: '[unused27]',\n"," 28: '[unused28]',\n"," 29: '[unused29]',\n"," 30: '[unused30]',\n"," 31: '[unused31]',\n"," 32: '[unused32]',\n"," 33: '[unused33]',\n"," 34: '[unused34]',\n"," 35: '[unused35]',\n"," 36: '[unused36]',\n"," 37: '[unused37]',\n"," 38: '[unused38]',\n"," 39: '[unused39]',\n"," 40: '[unused40]',\n"," 41: '[unused41]',\n"," 42: '[unused42]',\n"," 43: '[unused43]',\n"," 44: '[unused44]',\n"," 45: '[unused45]',\n"," 46: '[unused46]',\n"," 47: '[unused47]',\n"," 48: '[unused48]',\n"," 49: '[unused49]',\n"," 50: '[unused50]',\n"," 51: '[unused51]',\n"," 52: '[unused52]',\n"," 53: '[unused53]',\n"," 54: '[unused54]',\n"," 55: '[unused55]',\n"," 56: '[unused56]',\n"," 57: '[unused57]',\n"," 58: '[unused58]',\n"," 59: '[unused59]',\n"," 60: '[unused60]',\n"," 61: '[unused61]',\n"," 62: '[unused62]',\n"," 63: '[unused63]',\n"," 64: '[unused64]',\n"," 65: '[unused65]',\n"," 66: '[unused66]',\n"," 67: '[unused67]',\n"," 68: '[unused68]',\n"," 69: '[unused69]',\n"," 70: '[unused70]',\n"," 71: '[unused71]',\n"," 72: '[unused72]',\n"," 73: '[unused73]',\n"," 74: '[unused74]',\n"," 75: '[unused75]',\n"," 76: '[unused76]',\n"," 77: '[unused77]',\n"," 78: '[unused78]',\n"," 79: '[unused79]',\n"," 80: '[unused80]',\n"," 81: '[unused81]',\n"," 82: '[unused82]',\n"," 83: '[unused83]',\n"," 84: '[unused84]',\n"," 85: '[unused85]',\n"," 86: '[unused86]',\n"," 87: '[unused87]',\n"," 88: '[unused88]',\n"," 89: '[unused89]',\n"," 90: '[unused90]',\n"," 91: '[unused91]',\n"," 92: '[unused92]',\n"," 93: '[unused93]',\n"," 94: '[unused94]',\n"," 95: '[unused95]',\n"," 96: '[unused96]',\n"," 97: '[unused97]',\n"," 98: '[unused98]',\n"," 99: '[unused99]',\n"," 100: '[UNK]',\n"," 101: '[CLS]',\n"," 102: '[SEP]',\n"," 103: '[MASK]',\n"," 104: '<S>',\n"," 105: '<T>',\n"," 106: '!',\n"," 107: '\"',\n"," 108: '#',\n"," 109: '$',\n"," 110: '%',\n"," 111: '&',\n"," 112: \"'\",\n"," 113: '(',\n"," 114: ')',\n"," 115: '*',\n"," 116: '+',\n"," 117: ',',\n"," 118: '-',\n"," 119: '.',\n"," 120: '/',\n"," 121: '0',\n"," 122: '1',\n"," 123: '2',\n"," 124: '3',\n"," 125: '4',\n"," 126: '5',\n"," 127: '6',\n"," 128: '7',\n"," 129: '8',\n"," 130: '9',\n"," 131: ':',\n"," 132: ';',\n"," 133: '<',\n"," 134: '=',\n"," 135: '>',\n"," 136: '?',\n"," 137: '@',\n"," 138: 'A',\n"," 139: 'B',\n"," 140: 'C',\n"," 141: 'D',\n"," 142: 'E',\n"," 143: 'F',\n"," 144: 'G',\n"," 145: 'H',\n"," 146: 'I',\n"," 147: 'J',\n"," 148: 'K',\n"," 149: 'L',\n"," 150: 'M',\n"," 151: 'N',\n"," 152: 'O',\n"," 153: 'P',\n"," 154: 'Q',\n"," 155: 'R',\n"," 156: 'S',\n"," 157: 'T',\n"," 158: 'U',\n"," 159: 'V',\n"," 160: 'W',\n"," 161: 'X',\n"," 162: 'Y',\n"," 163: 'Z',\n"," 164: '[',\n"," 165: '\\\\',\n"," 166: ']',\n"," 167: '^',\n"," 168: '##',\n"," 169: 'a',\n"," 170: 'b',\n"," 171: 'c',\n"," 172: 'd',\n"," 173: 'e',\n"," 174: 'f',\n"," 175: 'g',\n"," 176: 'h',\n"," 177: 'i',\n"," 178: 'j',\n"," 179: 'k',\n"," 180: 'l',\n"," 181: 'm',\n"," 182: 'n',\n"," 183: 'o',\n"," 184: 'p',\n"," 185: 'q',\n"," 186: 'r',\n"," 187: 's',\n"," 188: 't',\n"," 189: 'u',\n"," 190: 'v',\n"," 191: 'w',\n"," 192: 'x',\n"," 193: 'y',\n"," 194: 'z',\n"," 195: '{',\n"," 196: '|',\n"," 197: '}',\n"," 198: '~',\n"," 199: '¡',\n"," 200: '¢',\n"," 201: '£',\n"," 202: '¥',\n"," 203: '¦',\n"," 204: '§',\n"," 205: '¨',\n"," 206: '©',\n"," 207: 'ª',\n"," 208: '«',\n"," 209: '¬',\n"," 210: '®',\n"," 211: '°',\n"," 212: '±',\n"," 213: '²',\n"," 214: '³',\n"," 215: 'µ',\n"," 216: '¶',\n"," 217: '·',\n"," 218: '¹',\n"," 219: 'º',\n"," 220: '»',\n"," 221: '¼',\n"," 222: '½',\n"," 223: '¾',\n"," 224: '¿',\n"," 225: 'À',\n"," 226: 'Á',\n"," 227: 'Â',\n"," 228: 'Ã',\n"," 229: 'Ä',\n"," 230: 'Å',\n"," 231: 'Æ',\n"," 232: 'Ç',\n"," 233: 'È',\n"," 234: 'É',\n"," 235: 'Ê',\n"," 236: 'Ë',\n"," 237: 'Ì',\n"," 238: 'Í',\n"," 239: 'Î',\n"," 240: 'Ð',\n"," 241: 'Ñ',\n"," 242: 'Ò',\n"," 243: 'Ó',\n"," 244: 'Ô',\n"," 245: 'Õ',\n"," 246: 'Ö',\n"," 247: '×',\n"," 248: 'Ø',\n"," 249: 'Ú',\n"," 250: 'Ü',\n"," 251: 'Ý',\n"," 252: 'Þ',\n"," 253: 'ß',\n"," 254: 'à',\n"," 255: 'á',\n"," 256: 'â',\n"," 257: 'ã',\n"," 258: 'ä',\n"," 259: 'å',\n"," 260: 'æ',\n"," 261: 'ç',\n"," 262: 'è',\n"," 263: 'é',\n"," 264: 'ê',\n"," 265: 'ë',\n"," 266: 'ì',\n"," 267: 'í',\n"," 268: 'î',\n"," 269: 'ï',\n"," 270: 'ð',\n"," 271: 'ñ',\n"," 272: 'ò',\n"," 273: 'ó',\n"," 274: 'ô',\n"," 275: 'õ',\n"," 276: 'ö',\n"," 277: '÷',\n"," 278: 'ø',\n"," 279: 'ù',\n"," 280: 'ú',\n"," 281: 'û',\n"," 282: 'ü',\n"," 283: 'ý',\n"," 284: 'þ',\n"," 285: 'ÿ',\n"," 286: 'Ā',\n"," 287: 'ā',\n"," 288: 'Ă',\n"," 289: 'ă',\n"," 290: 'Ą',\n"," 291: 'ą',\n"," 292: 'Ć',\n"," 293: 'ć',\n"," 294: 'Č',\n"," 295: 'č',\n"," 296: 'Ď',\n"," 297: 'ď',\n"," 298: 'Đ',\n"," 299: 'đ',\n"," 300: 'Ē',\n"," 301: 'ē',\n"," 302: 'Ĕ',\n"," 303: 'ĕ',\n"," 304: 'Ė',\n"," 305: 'ė',\n"," 306: 'ę',\n"," 307: 'ě',\n"," 308: 'Ğ',\n"," 309: 'ğ',\n"," 310: 'ġ',\n"," 311: 'Ģ',\n"," 312: 'ģ',\n"," 313: 'Ħ',\n"," 314: 'ħ',\n"," 315: 'ĩ',\n"," 316: 'Ī',\n"," 317: 'ī',\n"," 318: 'Į',\n"," 319: 'į',\n"," 320: 'İ',\n"," 321: 'ı',\n"," 322: 'Ķ',\n"," 323: 'ķ',\n"," 324: 'ĺ',\n"," 325: 'Ļ',\n"," 326: 'ļ',\n"," 327: 'Ľ',\n"," 328: 'ľ',\n"," 329: 'Ł',\n"," 330: 'ł',\n"," 331: 'ń',\n"," 332: 'Ņ',\n"," 333: 'ņ',\n"," 334: 'ň',\n"," 335: 'ŉ',\n"," 336: 'ŋ',\n"," 337: 'Ō',\n"," 338: 'ō',\n"," 339: 'ŏ',\n"," 340: 'Ő',\n"," 341: 'ő',\n"," 342: 'Œ',\n"," 343: 'œ',\n"," 344: 'ŕ',\n"," 345: 'Ř',\n"," 346: 'ř',\n"," 347: 'Ś',\n"," 348: 'ś',\n"," 349: 'Ş',\n"," 350: 'ş',\n"," 351: 'Š',\n"," 352: 'š',\n"," 353: 'Ţ',\n"," 354: 'ţ',\n"," 355: 'Ť',\n"," 356: 'ť',\n"," 357: 'ũ',\n"," 358: 'Ū',\n"," 359: 'ū',\n"," 360: 'ŭ',\n"," 361: 'ů',\n"," 362: 'Ű',\n"," 363: 'ű',\n"," 364: 'ų',\n"," 365: 'ŵ',\n"," 366: 'ŷ',\n"," 367: 'Ź',\n"," 368: 'ź',\n"," 369: 'Ż',\n"," 370: 'ż',\n"," 371: 'Ž',\n"," 372: 'ž',\n"," 373: 'Ə',\n"," 374: 'ƒ',\n"," 375: 'ơ',\n"," 376: 'Ư',\n"," 377: 'ư',\n"," 378: 'ǎ',\n"," 379: 'ǐ',\n"," 380: 'ǔ',\n"," 381: 'ǫ',\n"," 382: 'ǹ',\n"," 383: 'Ș',\n"," 384: 'ș',\n"," 385: 'Ț',\n"," 386: 'ț',\n"," 387: 'ɐ',\n"," 388: 'ɑ',\n"," 389: 'ɔ',\n"," 390: 'ɕ',\n"," 391: 'ə',\n"," 392: 'ɛ',\n"," 393: 'ɡ',\n"," 394: 'ɣ',\n"," 395: 'ɨ',\n"," 396: 'ɪ',\n"," 397: 'ɲ',\n"," 398: 'ɾ',\n"," 399: 'ʁ',\n"," 400: 'ʃ',\n"," 401: 'ʊ',\n"," 402: 'ʎ',\n"," 403: 'ʒ',\n"," 404: 'ʔ',\n"," 405: 'ʙ',\n"," 406: 'ʰ',\n"," 407: 'ʲ',\n"," 408: 'ʳ',\n"," 409: 'ʷ',\n"," 410: 'ʸ',\n"," 411: 'ʻ',\n"," 412: 'ʼ',\n"," 413: 'ʾ',\n"," 414: 'ʿ',\n"," 415: 'ˈ',\n"," 416: 'ː',\n"," 417: 'ˡ',\n"," 418: 'ˢ',\n"," 419: '̀',\n"," 420: '́',\n"," 421: '̃',\n"," 422: '̄',\n"," 423: '̍',\n"," 424: '̥',\n"," 425: '̧',\n"," 426: '̲',\n"," 427: '͡',\n"," 428: '΄',\n"," 429: 'Ά',\n"," 430: 'Έ',\n"," 431: 'Ή',\n"," 432: 'Ί',\n"," 433: 'Ό',\n"," 434: 'Ύ',\n"," 435: 'Ώ',\n"," 436: 'ΐ',\n"," 437: 'Α',\n"," 438: 'Β',\n"," 439: 'Γ',\n"," 440: 'Δ',\n"," 441: 'Ε',\n"," 442: 'Ζ',\n"," 443: 'Η',\n"," 444: 'Θ',\n"," 445: 'Ι',\n"," 446: 'Κ',\n"," 447: 'Λ',\n"," 448: 'Μ',\n"," 449: 'Ν',\n"," 450: 'Ξ',\n"," 451: 'Ο',\n"," 452: 'Π',\n"," 453: 'Ρ',\n"," 454: 'Σ',\n"," 455: 'Τ',\n"," 456: 'Υ',\n"," 457: 'Φ',\n"," 458: 'Χ',\n"," 459: 'Ψ',\n"," 460: 'Ω',\n"," 461: 'ά',\n"," 462: 'έ',\n"," 463: 'ή',\n"," 464: 'ί',\n"," 465: 'α',\n"," 466: 'β',\n"," 467: 'γ',\n"," 468: 'δ',\n"," 469: 'ε',\n"," 470: 'ζ',\n"," 471: 'η',\n"," 472: 'θ',\n"," 473: 'ι',\n"," 474: 'κ',\n"," 475: 'λ',\n"," 476: 'μ',\n"," 477: 'ν',\n"," 478: 'ξ',\n"," 479: 'ο',\n"," 480: 'π',\n"," 481: 'ρ',\n"," 482: 'ς',\n"," 483: 'σ',\n"," 484: 'τ',\n"," 485: 'υ',\n"," 486: 'φ',\n"," 487: 'χ',\n"," 488: 'ψ',\n"," 489: 'ω',\n"," 490: 'ϊ',\n"," 491: 'ϋ',\n"," 492: 'ό',\n"," 493: 'ύ',\n"," 494: 'ώ',\n"," 495: 'Ё',\n"," 496: 'Ђ',\n"," 497: 'Ѓ',\n"," 498: 'Є',\n"," 499: 'Ѕ',\n"," 500: 'І',\n"," 501: 'Ї',\n"," 502: 'Ј',\n"," 503: 'Љ',\n"," 504: 'Њ',\n"," 505: 'Ћ',\n"," 506: 'Ќ',\n"," 507: 'Ў',\n"," 508: 'Џ',\n"," 509: 'А',\n"," 510: 'Б',\n"," 511: 'В',\n"," 512: 'Г',\n"," 513: 'Д',\n"," 514: 'Е',\n"," 515: 'Ж',\n"," 516: 'З',\n"," 517: 'И',\n"," 518: 'Й',\n"," 519: 'К',\n"," 520: 'Л',\n"," 521: 'М',\n"," 522: 'Н',\n"," 523: 'О',\n"," 524: 'П',\n"," 525: 'Р',\n"," 526: 'С',\n"," 527: 'Т',\n"," 528: 'У',\n"," 529: 'Ф',\n"," 530: 'Х',\n"," 531: 'Ц',\n"," 532: 'Ч',\n"," 533: 'Ш',\n"," 534: 'Щ',\n"," 535: 'Ъ',\n"," 536: 'Ы',\n"," 537: 'Ь',\n"," 538: 'Э',\n"," 539: 'Ю',\n"," 540: 'Я',\n"," 541: 'а',\n"," 542: 'б',\n"," 543: 'в',\n"," 544: 'г',\n"," 545: 'д',\n"," 546: 'е',\n"," 547: 'ж',\n"," 548: 'з',\n"," 549: 'и',\n"," 550: 'й',\n"," 551: 'к',\n"," 552: 'л',\n"," 553: 'м',\n"," 554: 'н',\n"," 555: 'о',\n"," 556: 'п',\n"," 557: 'р',\n"," 558: 'с',\n"," 559: 'т',\n"," 560: 'у',\n"," 561: 'ф',\n"," 562: 'х',\n"," 563: 'ц',\n"," 564: 'ч',\n"," 565: 'ш',\n"," 566: 'щ',\n"," 567: 'ъ',\n"," 568: 'ы',\n"," 569: 'ь',\n"," 570: 'э',\n"," 571: 'ю',\n"," 572: 'я',\n"," 573: 'ѐ',\n"," 574: 'ё',\n"," 575: 'ђ',\n"," 576: 'ѓ',\n"," 577: 'є',\n"," 578: 'ѕ',\n"," 579: 'і',\n"," 580: 'ї',\n"," 581: 'ј',\n"," 582: 'љ',\n"," 583: 'њ',\n"," 584: 'ћ',\n"," 585: 'ќ',\n"," 586: 'ѝ',\n"," 587: 'ў',\n"," 588: 'џ',\n"," 589: 'ѣ',\n"," 590: 'Ґ',\n"," 591: 'ґ',\n"," 592: 'Ғ',\n"," 593: 'ғ',\n"," 594: 'Җ',\n"," 595: 'җ',\n"," 596: 'Ҙ',\n"," 597: 'ҙ',\n"," 598: 'Қ',\n"," 599: 'қ',\n"," 600: 'Ҡ',\n"," 601: 'ҡ',\n"," 602: 'ң',\n"," 603: 'ҫ',\n"," 604: 'Ү',\n"," 605: 'ү',\n"," 606: 'Ұ',\n"," 607: 'ұ',\n"," 608: 'Ҳ',\n"," 609: 'ҳ',\n"," 610: 'Ҷ',\n"," 611: 'ҷ',\n"," 612: 'Һ',\n"," 613: 'һ',\n"," 614: 'Ӏ',\n"," 615: 'ӑ',\n"," 616: 'ӗ',\n"," 617: 'Ә',\n"," 618: 'ә',\n"," 619: 'ӣ',\n"," 620: 'Ө',\n"," 621: 'ө',\n"," 622: 'Ӯ',\n"," 623: 'ӯ',\n"," 624: 'ӳ',\n"," 625: 'Ա',\n"," 626: 'Բ',\n"," 627: 'Գ',\n"," 628: 'Դ',\n"," 629: 'Ե',\n"," 630: 'Զ',\n"," 631: 'Է',\n"," 632: 'Ը',\n"," 633: 'Թ',\n"," 634: 'Ժ',\n"," 635: 'Ի',\n"," 636: 'Լ',\n"," 637: 'Խ',\n"," 638: 'Ծ',\n"," 639: 'Կ',\n"," 640: 'Հ',\n"," 641: 'Ձ',\n"," 642: 'Ղ',\n"," 643: 'Ճ',\n"," 644: 'Մ',\n"," 645: 'Յ',\n"," 646: 'Ն',\n"," 647: 'Շ',\n"," 648: 'Ո',\n"," 649: 'Չ',\n"," 650: 'Պ',\n"," 651: 'Ջ',\n"," 652: 'Ռ',\n"," 653: 'Ս',\n"," 654: 'Վ',\n"," 655: 'Տ',\n"," 656: 'Ր',\n"," 657: 'Ց',\n"," 658: 'Ւ',\n"," 659: 'Փ',\n"," 660: 'Ք',\n"," 661: 'Օ',\n"," 662: 'Ֆ',\n"," 663: '՚',\n"," 664: '՛',\n"," 665: '՜',\n"," 666: '՝',\n"," 667: '՞',\n"," 668: 'ա',\n"," 669: 'բ',\n"," 670: 'գ',\n"," 671: 'դ',\n"," 672: 'ե',\n"," 673: 'զ',\n"," 674: 'է',\n"," 675: 'ը',\n"," 676: 'թ',\n"," 677: 'ժ',\n"," 678: 'ի',\n"," 679: 'լ',\n"," 680: 'խ',\n"," 681: 'ծ',\n"," 682: 'կ',\n"," 683: 'հ',\n"," 684: 'ձ',\n"," 685: 'ղ',\n"," 686: 'ճ',\n"," 687: 'մ',\n"," 688: 'յ',\n"," 689: 'ն',\n"," 690: 'շ',\n"," 691: 'ո',\n"," 692: 'չ',\n"," 693: 'պ',\n"," 694: 'ջ',\n"," 695: 'ռ',\n"," 696: 'ս',\n"," 697: 'վ',\n"," 698: 'տ',\n"," 699: 'ր',\n"," 700: 'ց',\n"," 701: 'ւ',\n"," 702: 'փ',\n"," 703: 'ք',\n"," 704: 'օ',\n"," 705: 'ֆ',\n"," 706: 'և',\n"," 707: '։',\n"," 708: '֊',\n"," 709: 'ְ',\n"," 710: 'ֱ',\n"," 711: 'ֲ',\n"," 712: 'ִ',\n"," 713: 'ֵ',\n"," 714: 'ֶ',\n"," 715: 'ַ',\n"," 716: 'ָ',\n"," 717: 'ֹ',\n"," 718: 'ּ',\n"," 719: '־',\n"," 720: 'ׁ',\n"," 721: 'ׂ',\n"," 722: '׃',\n"," 723: 'א',\n"," 724: 'ב',\n"," 725: 'ג',\n"," 726: 'ד',\n"," 727: 'ה',\n"," 728: 'ו',\n"," 729: 'ז',\n"," 730: 'ח',\n"," 731: 'ט',\n"," 732: 'י',\n"," 733: 'ך',\n"," 734: 'כ',\n"," 735: 'ל',\n"," 736: 'ם',\n"," 737: 'מ',\n"," 738: 'ן',\n"," 739: 'נ',\n"," 740: 'ס',\n"," 741: 'ע',\n"," 742: 'ף',\n"," 743: 'פ',\n"," 744: 'ץ',\n"," 745: 'צ',\n"," 746: 'ק',\n"," 747: 'ר',\n"," 748: 'ש',\n"," 749: 'ת',\n"," 750: '׳',\n"," 751: '״',\n"," 752: '،',\n"," 753: '؍',\n"," 754: 'ؒ',\n"," 755: '؛',\n"," 756: '؟',\n"," 757: 'ء',\n"," 758: 'آ',\n"," 759: 'أ',\n"," 760: 'ؤ',\n"," 761: 'إ',\n"," 762: 'ئ',\n"," 763: 'ا',\n"," 764: 'ب',\n"," 765: 'ة',\n"," 766: 'ت',\n"," 767: 'ث',\n"," 768: 'ج',\n"," 769: 'ح',\n"," 770: 'خ',\n"," 771: 'د',\n"," 772: 'ذ',\n"," 773: 'ر',\n"," 774: 'ز',\n"," 775: 'س',\n"," 776: 'ش',\n"," 777: 'ص',\n"," 778: 'ض',\n"," 779: 'ط',\n"," 780: 'ظ',\n"," 781: 'ع',\n"," 782: 'غ',\n"," 783: 'ـ',\n"," 784: 'ف',\n"," 785: 'ق',\n"," 786: 'ك',\n"," 787: 'ل',\n"," 788: 'م',\n"," 789: 'ن',\n"," 790: 'ه',\n"," 791: 'و',\n"," 792: 'ى',\n"," 793: 'ي',\n"," 794: 'ً',\n"," 795: 'ٌ',\n"," 796: 'ٍ',\n"," 797: 'َ',\n"," 798: 'ُ',\n"," 799: 'ِ',\n"," 800: 'ّ',\n"," 801: 'ْ',\n"," 802: 'ٔ',\n"," 803: 'ٛ',\n"," 804: '٠',\n"," 805: '١',\n"," 806: '٢',\n"," 807: '٣',\n"," 808: '٤',\n"," 809: '٥',\n"," 810: '٩',\n"," 811: '٪',\n"," 812: '٫',\n"," 813: '٬',\n"," 814: '٭',\n"," 815: 'ٰ',\n"," 816: 'ٹ',\n"," 817: 'پ',\n"," 818: 'چ',\n"," 819: 'ڈ',\n"," 820: 'ڑ',\n"," 821: 'ژ',\n"," 822: 'ڤ',\n"," 823: 'ک',\n"," 824: 'ڭ',\n"," 825: 'گ',\n"," 826: 'ں',\n"," 827: 'ھ',\n"," 828: 'ۀ',\n"," 829: 'ہ',\n"," 830: 'ۂ',\n"," 831: 'ۃ',\n"," 832: 'ۆ',\n"," 833: 'ۇ',\n"," 834: 'ی',\n"," 835: 'ے',\n"," 836: 'ۓ',\n"," 837: '۔',\n"," 838: '۰',\n"," 839: '۱',\n"," 840: '۲',\n"," 841: '۳',\n"," 842: '۴',\n"," 843: '۵',\n"," 844: '۶',\n"," 845: '۷',\n"," 846: '۸',\n"," 847: '۹',\n"," 848: 'ँ',\n"," 849: 'ं',\n"," 850: 'ः',\n"," 851: 'अ',\n"," 852: 'आ',\n"," 853: 'इ',\n"," 854: 'ई',\n"," 855: 'उ',\n"," 856: 'ऊ',\n"," 857: 'ऋ',\n"," 858: 'ऍ',\n"," 859: 'ऎ',\n"," 860: 'ए',\n"," 861: 'ऐ',\n"," 862: 'ऑ',\n"," 863: 'ओ',\n"," 864: 'औ',\n"," 865: 'क',\n"," 866: 'ख',\n"," 867: 'ग',\n"," 868: 'घ',\n"," 869: 'ङ',\n"," 870: 'च',\n"," 871: 'छ',\n"," 872: 'ज',\n"," 873: 'झ',\n"," 874: 'ञ',\n"," 875: 'ट',\n"," 876: 'ठ',\n"," 877: 'ड',\n"," 878: 'ढ',\n"," 879: 'ण',\n"," 880: 'त',\n"," 881: 'थ',\n"," 882: 'द',\n"," 883: 'ध',\n"," 884: 'न',\n"," 885: 'प',\n"," 886: 'फ',\n"," 887: 'ब',\n"," 888: 'भ',\n"," 889: 'म',\n"," 890: 'य',\n"," 891: 'र',\n"," 892: 'ऱ',\n"," 893: 'ल',\n"," 894: 'ळ',\n"," 895: 'व',\n"," 896: 'श',\n"," 897: 'ष',\n"," 898: 'स',\n"," 899: 'ह',\n"," 900: '़',\n"," 901: 'ऽ',\n"," 902: 'ा',\n"," 903: 'ि',\n"," 904: 'ी',\n"," 905: 'ु',\n"," 906: 'ू',\n"," 907: 'ृ',\n"," 908: 'ॄ',\n"," 909: 'ॅ',\n"," 910: 'ॆ',\n"," 911: 'े',\n"," 912: 'ै',\n"," 913: 'ॉ',\n"," 914: 'ो',\n"," 915: 'ौ',\n"," 916: '्',\n"," 917: 'ॐ',\n"," 918: '॓',\n"," 919: 'ॠ',\n"," 920: '।',\n"," 921: '॥',\n"," 922: '०',\n"," 923: '१',\n"," 924: '२',\n"," 925: '३',\n"," 926: '४',\n"," 927: '५',\n"," 928: '६',\n"," 929: '७',\n"," 930: '८',\n"," 931: '९',\n"," 932: '॰',\n"," 933: 'ॲ',\n"," 934: 'ঁ',\n"," 935: 'ং',\n"," 936: 'ঃ',\n"," 937: 'অ',\n"," 938: 'আ',\n"," 939: 'ই',\n"," 940: 'ঈ',\n"," 941: 'উ',\n"," 942: 'ঊ',\n"," 943: 'ঋ',\n"," 944: 'এ',\n"," 945: 'ঐ',\n"," 946: 'ও',\n"," 947: 'ঔ',\n"," 948: 'ক',\n"," 949: 'খ',\n"," 950: 'গ',\n"," 951: 'ঘ',\n"," 952: 'ঙ',\n"," 953: 'চ',\n"," 954: 'ছ',\n"," 955: 'জ',\n"," 956: 'ঝ',\n"," 957: 'ঞ',\n"," 958: 'ট',\n"," 959: 'ঠ',\n"," 960: 'ড',\n"," 961: 'ঢ',\n"," 962: 'ণ',\n"," 963: 'ত',\n"," 964: 'থ',\n"," 965: 'দ',\n"," 966: 'ধ',\n"," 967: 'ন',\n"," 968: 'প',\n"," 969: 'ফ',\n"," 970: 'ব',\n"," 971: 'ভ',\n"," 972: 'ম',\n"," 973: 'য',\n"," 974: 'র',\n"," 975: 'ল',\n"," 976: 'শ',\n"," 977: 'ষ',\n"," 978: 'স',\n"," 979: 'হ',\n"," 980: '়',\n"," 981: 'া',\n"," 982: 'ি',\n"," 983: 'ী',\n"," 984: 'ু',\n"," 985: 'ূ',\n"," 986: 'ৃ',\n"," 987: 'ে',\n"," 988: 'ৈ',\n"," 989: 'ো',\n"," 990: 'ৌ',\n"," 991: '্',\n"," 992: 'ৎ',\n"," 993: '০',\n"," 994: '১',\n"," 995: '২',\n"," 996: '৩',\n"," 997: '৪',\n"," 998: '৫',\n"," 999: '৬',\n"," ...}"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"I9sFKLjD-v2n"},"source":["**버트 모형에 들어갈 인풋은 토큰, 세그먼트, 포지션으로 구성됩니다.**  \n","버트에 인풋으로 들어가는 토큰은 문장을 토크나이징 한 후, 인덱스 번호를 매긴 것입니다.  \n","세그먼트는 예를 들어 문장이 두 개가 있다면, 앞의 문장과 뒤의 문장을 구분하는 것입니다.  \n","포지션 임베딩은 단순히 단어의 위치를 말합니다.\n","\n","토큰, 세그먼트, 포지션을 인풋으로 버트 모형에 넣으면 기하학적인 문장 공간으로 임베딩이 됩니다.  \n","그림을 보면 my dog is cute, he likes play ##ing 두 문장이 있는데요  \n","SQUAD 문제에서는 첫번째 문장이 질문, 두번째 문장이 context가 되고, 이 두 문장이 합쳐져서 하나의 문장으로 들어가게 됩니다.\n","\n","![대체 텍스트](https://i.imgur.com/l9BTao3.png)"]},{"cell_type":"markdown","metadata":{"id":"oXdjhy0cLlYA"},"source":["- 사전학습된 버트 모델의 인풋은 문장 토큰화가 숫자로 바뀐 것과, 앞문장인지 뒷문장인지 알려주는 문장 순서 벡터가 들어갑니다. 우리는 문장 하나를 가지고만 훈련할 것이므로 순서 벡터는 모두 0으로 통일합니다.\n","\n","- 그리고 파인튜닝 시에는 문장 안에 일부 단어를 가리는 마스킹은 사용하지 않습니다."]},{"cell_type":"markdown","metadata":{"id":"Fraw-fe9PL1k"},"source":["우리가 로드하였던 SQUAD 데이터를 **버트 모형의 입력에 맞게 변형해주는 함수**를 정의하도록 하겠습니다.\n","\n","함수 내부에 tokenizer.encode 함수가 버트 모형을 토큰화해주고 토큰화 된 단어를 인덱스에 맞게 숫자로 바꿔주게 됩니다.  \n","**[CLS] 질문 [SEP] 문장 [SEP]** 이런 방식으로 질문과 문장이 인풋으로 들어가게 됩니다. SEQ_LEN이 384로 지정되어 있어서 길이가 384가 넘는 인풋은 문장 부분이 잘려서 들어가게 됩니다.  \n","SQUAD 문제에서 문장(context) 내에 text(정답)이 포함된다고 미리 말씀 드렸는데요, 길이가 384가 넘는 인풋인 경우에 context 내에 정답을 포함하고 있는 context가 잘려서 정답을 포함하지 않는 경우가 생깁니다. 이번 실습에서는 이러한 경우의 인덱스를 del_list로 지정해서, 빼도록 하겠습니다. (숙제로 남겨 두겠습니다.) "]},{"cell_type":"code","metadata":{"id":"jYrlZPd3YExo"},"source":["question = train['question'][0]\n","context = train['context'][0]\n","text = train['text'][0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xjcdonPiY3U9","executionInfo":{"status":"ok","timestamp":1601723279243,"user_tz":-540,"elapsed":997,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"52b25c96-bdb4-4a0d-c444-37732a02be4c","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# 질문\n","question"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?'"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"2tRbEbSCY4dx","executionInfo":{"status":"ok","timestamp":1601723281368,"user_tz":-540,"elapsed":979,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"fdd7bc51-b45d-48d0-9822-0e793a9ed16c","colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["# 문장\n","context"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.'"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"id":"ZhMGIFOhY5hz","executionInfo":{"status":"ok","timestamp":1601723283451,"user_tz":-540,"elapsed":1023,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"81cc04e8-ed48-4642-a4bd-a4ce60b3443b","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["# 정답\n","text"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'Saint Bernadette Soubirous'"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"10hBmhcvY68i","executionInfo":{"status":"ok","timestamp":1601723285379,"user_tz":-540,"elapsed":1002,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"8922f303-2cff-43fd-94f2-94d23072d0bf","colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["print(tokenizer.tokenize(question, context))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['[CLS]', 'to', 'whom', 'did', 'the', 'vir', '##gin', 'mar', '##y', 'allegedly', 'appear', 'in', '1858', 'in', 'lo', '##urde', '##s', 'franc', '##e', '?', '[SEP]', 'architectural', '##ly', ',', 'the', 'school', 'has', 'a', 'cat', '##hol', '##ic', 'character', '.', 'ato', '##p', 'the', 'main', 'building', \"'\", 's', 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'vir', '##gin', 'mar', '##y', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'ch', '##rist', 'with', 'arms', 'up', '##rais', '##ed', 'with', 'the', 'legend', '\"', 'ven', '##ite', 'ad', 'me', 'om', '##nes', '\"', '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'gr', '##otto', ',', 'a', 'mari', '##an', 'place', 'of', 'prayer', 'and', 'reflect', '##ion', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'gr', '##otto', 'at', 'lo', '##urde', '##s', ',', 'franc', '##e', 'where', 'the', 'vir', '##gin', 'mar', '##y', 'rep', '##uted', '##ly', 'appeared', 'to', 'saint', 'bern', '##ade', '##tte', 'sou', '##bir', '##ous', 'in', '1858', '.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mar', '##y', '.', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SvahjcksZIN9","executionInfo":{"status":"ok","timestamp":1601723290089,"user_tz":-540,"elapsed":976,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"7d92b56d-58dd-4d44-c12c-d929a0e2dedc","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print(tokenizer.tokenize(text))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['[CLS]', 'saint', 'bern', '##ade', '##tte', 'sou', '##bir', '##ous', '[SEP]']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"fgOlJH_4ZQpS"},"source":["우리의 목표는, 질문(question)과 문장(context)를 받아서, 정답(text)를 맞추는 모델을 만드는 것입니다.  \n","정답을 통째로 맞추는 것이 아니라, **토큰화된 것의 맨 앞 단어와, 맨 뒷 단어입니다.**  \n","토큰화된 정답은 ['[CLS]', 'saint', 'bern', '##ade', '##tte', 'sou', '##bir', '##ous', '[SEP]'] 인데, 여기서 **saint**에 해당하는 위치와 **##ous**에 해당하는 위치를 맞추는 버트 모형을 파인튜닝 하려 하는 것입니다.  \n","\n","  그래서 밑에 convert_data 함수에서, 정답(text) 길이만큼 문장(context)를 슬라이딩 하면서 만약에 문장이 정답을 포함하는 위치에 도달하면, 문장에서 정답의 맨 앞이 우리가 예측할 1번째 정답, 정답의 맨 뒤가 우리가 예측할 2번째 정답이 되게 됩니다."]},{"cell_type":"code","metadata":{"id":"0rJsxRZDWw4e"},"source":["def convert_data(data_df):\n","    global tokenizer\n","    indices, segments, target_start, target_end = [], [], [], []\n","    for i in tqdm(range(len(data_df))):\n","        \n","        ids, segment = tokenizer.encode(data_df[QUESTION_COLUMN][i], data_df[DATA_COLUMN][i], max_len=SEQ_LEN)\n","        \n","\n","        text = tokenizer.encode(data_df[TEXT][i])[0]\n","\n","        text_slide_len = len(text[1:-1])\n","        for i in range(1,len(ids)-text_slide_len-1):  \n","            exist_flag = 0\n","            if text[1:-1] == ids[i:i+text_slide_len]:\n","              ans_start = i\n","              ans_end = i + text_slide_len - 1\n","              exist_flag = 1\n","              break\n","        \n","        if exist_flag == 0:\n","          ans_start = SEQ_LEN\n","          ans_end = SEQ_LEN\n","\n","        indices.append(ids)\n","        segments.append(segment)\n","\n","        target_start.append(ans_start)\n","        target_end.append(ans_end)\n","\n","    indices_x = np.array(indices)\n","    segments = np.array(segments)\n","    target_start = np.array(target_start)\n","    target_end = np.array(target_end)\n","    \n","    del_list = np.where(target_start!=SEQ_LEN)[0]\n","\n","    indices_x = indices_x[del_list]\n","    segments = segments[del_list]\n","    target_start = target_start[del_list]\n","    target_end = target_end[del_list]\n","\n","    train_y_0 = keras.utils.to_categorical(target_start, num_classes=SEQ_LEN, dtype='int64')\n","    train_y_1 = keras.utils.to_categorical(target_end, num_classes=SEQ_LEN, dtype='int64')\n","    train_y_cat = [train_y_0, train_y_1]\n","    \n","    return [indices_x, segments], train_y_cat\n","\n","def load_data(pandas_dataframe):\n","    data_df = pandas_dataframe\n","    \n","    \n","    data_df[DATA_COLUMN] = data_df[DATA_COLUMN].astype(str)\n","    data_df[QUESTION_COLUMN] = data_df[QUESTION_COLUMN].astype(str)\n","\n","\n","    data_x, data_y = convert_data(data_df)\n","\n","    return data_x, data_y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rNUYX-Rntsk_","executionInfo":{"status":"ok","timestamp":1601723476982,"user_tz":-540,"elapsed":177785,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"e6d4edab-eaaf-4bfb-8bcc-9552a62a8a0c","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_x, train_y = load_data(train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["100%|██████████| 87599/87599 [02:49<00:00, 516.91it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"vnh0ZuikSzz6"},"source":["#### 이해가 안 가실 수 있는데, 버트 인풋을 문장으로 예를 들어 만들어 보겠습니다.\n","#### 인풋은 총 2개가 들어갑니다\n","- **(토큰)** 첫번째 인풋은 토큰화 된 것이 인덱싱되어 숫자로 변환된 것  \n","\n","- **(세그멘트)** 두번째 인풋은 앞문장인지 뒷문장인지 알려주는 숫자들입니다. 이번 튜토리얼에서는 파인튜닝 과정이라 앞문장 뒷문장 구분을 안하기 때문에 모두 0으로 하였습니다.  \n","\n","- **(포지션)** 단어 순서에 따라서 자동으로 부여됩니다.\n"]},{"cell_type":"markdown","metadata":{"id":"RZ23W0s8MCBt"},"source":["이해가 되셨는지요?  \n","구글 깃허브에서 다운받았던 사전학습된 모델을 colab으로 로드합니다.  \n","Training을 False로 두어서 Bert 모델에서, 마지막 트랜스포머 계층까지만 모델이 로드되게 합니다."]},{"cell_type":"code","metadata":{"scrolled":false,"id":"8auSqxdHtslD","executionInfo":{"status":"error","timestamp":1601723576783,"user_tz":-540,"elapsed":1020,"user":{"displayName":"정인환","photoUrl":"","userId":"14508790529200005254"}},"outputId":"c0d0dc91-d4c7-418f-9d83-496f0efd8d31","colab":{"base_uri":"https://localhost:8080/","height":297}},"source":["layer_num = 12\n","model = load_trained_model_from_checkpoint(\n","    config_path,\n","    checkpoint_path,\n","    training=False,\n","    trainable=True,\n","    seq_len=SEQ_LEN,)\n","model.summary()\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-34-9ff90c36fa9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     seq_len=SEQ_LEN,)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_bert/loader.py\u001b[0m in \u001b[0;36mload_trained_model_from_checkpoint\u001b[0;34m(config_file, checkpoint_file, training, trainable, output_layer_num, seq_len, **kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0moutput_layer_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_layer_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mseq_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0mload_model_weights_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_bert/loader.py\u001b[0m in \u001b[0;36mbuild_model_from_config\u001b[0;34m(config_file, training, trainable, output_layer_num, seq_len, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0moutput_layer_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_layer_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_bert/bert.py\u001b[0m in \u001b[0;36mget_model\u001b[0;34m(token_num, pos_num, seq_len, embed_dim, transformer_num, head_num, feed_forward_dim, dropout_rate, attention_activation, feed_forward_activation, training, trainable, output_layer_num, use_task_embed, task_num)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mpos_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_num\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mdropout_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     )\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_task_embed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras_bert/layers/embedding.py\u001b[0m in \u001b[0;36mget_embedding\u001b[0;34m(inputs, token_num, pos_num, embed_dim, dropout_rate, trainable)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Embedding-Token'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         )(inputs[0]),\n\u001b[0m\u001b[1;32m     38\u001b[0m         keras.layers.Embedding(\n\u001b[1;32m     39\u001b[0m             \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_add_inbound_node\u001b[0;34m(self, input_tensors, output_tensors, input_masks, output_masks, input_shapes, output_shapes, arguments)\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"markdown","metadata":{"id":"_ptok2n0MHoV"},"source":["모델의 구조를 확인합니다.  \n","총 12층의 트랜스포머 계층이 있음을 확인할 수 있습니다.\n"]},{"cell_type":"markdown","metadata":{"id":"g1N8hZvNMMWL"},"source":["Transfer learning을 위해 Custom Layer를 작성해 줍니다.  \n","NonMasking 함수를 지정해서, Bert 모형의 자체 Masking 된 텐서들을 풀어줘야 합니다.  \n","이번 튜토리얼에서 만약 NonMasking 클래스를 만들지 않는다면, Bert 모형을 훈련할 수 없습니다."]},{"cell_type":"code","metadata":{"id":"H99O5l4X8xyb"},"source":["class NonMasking(Layer):   \n","    def __init__(self, **kwargs):   \n","        self.supports_masking = True  \n","        super(NonMasking, self).__init__(**kwargs)   \n","  \n","    def build(self, input_shape):   \n","        input_shape = input_shape   \n","  \n","    def compute_mask(self, input, input_mask=None):   \n","        return None   \n","  \n","    def call(self, x, mask=None):   \n","        return x   \n","  \n","    def get_output_shape_for(self, input_shape):   \n","        return input_shape  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0BgB9WtubcgQ"},"source":["Keras Custom Layer 두 개를 생성합니다.  \n","MyLayer_Start는 정답의 첫 번째 단어를 예측하는 것을 담당하고,  \n","MyLaer_End는 정답의 마지막 단어를 예측하는 것을 담당합니다.  \n","  \n","사실 두 레이어는 동일한 역할을 합니다.  \n","Bert 모형의 마지막 입력을 받아서, (batch_size, 384, 768)의 텐서 모양을 (batch_size, 384, 2)로 만들어주는 텐서를 곱해줍니다.  \n","이 다음에 i) (batch_size, 384), ii) (batch_size, 384)의 아웃풋을 출력할 수 있게 하나의 텐서를 두개로 잘라줍니다.  \n","  \n","왜 끝이 384냐면, 384개의 위치를 예측하기 때문입니다. 단어의 위치의 최대 개수는 384개로 앞서 지정하였습니다.(SEQ_LEN)\n"]},{"cell_type":"code","metadata":{"id":"gOBsah1UgIPq"},"source":["class MyLayer_Start(Layer):\n","\n","    def __init__(self,seq_len, **kwargs):\n","        \n","        self.seq_len = seq_len\n","        self.supports_masking = True\n","        super(MyLayer_Start, self).__init__(**kwargs)\n","\n","    def build(self, input_shape):\n","        \n","        self.W = self.add_weight(name='kernel', \n","                                 shape=(768,2),\n","                                 initializer='uniform',\n","                                 trainable=True)\n","        super(MyLayer_Start, self).build(input_shape)\n","\n","    def call(self, x):\n","        \n","        x = K.reshape(x, shape=(-1,384,768))\n","        x = K.dot(x, self.W)\n","        \n","        x = K.permute_dimensions(x, (2,0,1))\n","\n","        self.start_logits, self.end_logits = x[0], x[1]\n","        \n","        self.start_logits = K.softmax(self.start_logits, axis=-1)\n","        \n","        return self.start_logits\n","\n","    def compute_output_shape(self, input_shape):\n","        return (input_shape[0], self.seq_len)\n","\n","\n","class MyLayer_End(Layer):\n","  def __init__(self,seq_len, **kwargs):\n","        \n","        self.seq_len = seq_len\n","        self.supports_masking = True\n","        super(MyLayer_End, self).__init__(**kwargs)\n","  \n","  def build(self, input_shape):\n","        \n","        self.W = self.add_weight(name='kernel', \n","                                 shape=(768, 2),\n","                                 initializer='uniform',\n","                                 trainable=True)\n","        super(MyLayer_End, self).build(input_shape)\n","\n","  \n","  def call(self, x):\n","\n","        \n","        x = K.reshape(x, shape=(-1,384,768))\n","        x = K.dot(x, self.W)\n","        x = K.permute_dimensions(x, (2,0,1))\n","        \n","        self.start_logits, self.end_logits = x[0], x[1]\n","        \n","        self.end_logits = K.softmax(self.end_logits, axis=-1)\n","        \n","        return self.end_logits\n","\n","  def compute_output_shape(self, input_shape):\n","        return (input_shape[0], self.seq_len)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3uYTT6WYgiBG"},"source":["BERT 모델을 출력하는 함수를 지정합니다.  \n","start_answer, end_answer를 예측하게 됩니다."]},{"cell_type":"code","metadata":{"id":"NcbOzpEb0wxU"},"source":["from keras.layers import merge, dot, concatenate\n","from keras import metrics\n","def get_bert_finetuning_model(model):\n","  inputs = model.inputs[:2]\n","  dense = model.output\n","  x = NonMasking()(dense)\n","  outputs_start = MyLayer_Start(384)(x)\n","  outputs_end = MyLayer_End(384)(x)\n","  bert_model = keras.models.Model(inputs, [outputs_start, outputs_end])\n","  bert_model.compile(\n","      optimizer=RAdam(learning_rate=LR, decay=0.001),\n","      loss='categorical_crossentropy',\n","      metrics=['accuracy'])\n","  \n","  return bert_model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H2LAk6d5dm4A"},"source":["**모델의 FLOW를 확인해 보도록 하겠습니다.**"]},{"cell_type":"code","metadata":{"id":"scWw12ysbPZ9"},"source":["from IPython.display import SVG\n","from keras.utils import model_to_dot\n","\n","\n","SVG(model_to_dot(get_bert_finetuning_model(model), dpi=65).create(prog='dot', format='svg'))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gLLayijHNEjz"},"source":["훈련을 시작합니다.  \n","1epoch을 훈련해 보고, 결과를 확인하고 훈련을 다시 시작할 예정입니다."]},{"cell_type":"code","metadata":{"id":"-7On1hY3I7o1"},"source":["sess = K.get_session()\n","uninitialized_variables = set([i.decode('ascii') for i in sess.run(tf.report_uninitialized_variables())])\n","init = tf.variables_initializer([v for v in tf.global_variables() if v.name.split(':')[0] in uninitialized_variables])\n","sess.run(init)\n","\n","bert_model = get_bert_finetuning_model(model)\n","bert_model.summary()\n","history = bert_model.fit(train_x, train_y, batch_size=10, validation_split=0.05, shuffle=False, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RSKZXlpFox6_"},"source":["g드라이브상에 squad 모델을 저장할 경로 지정"]},{"cell_type":"code","metadata":{"id":"qhW2hIzQoOtf"},"source":["path = \"gdrive/My Drive/Colab Notebooks/squad\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xC96iEmbhKZM"},"source":["BERT MODEL을 저장합니다."]},{"cell_type":"code","metadata":{"id":"E3pc5zhjCet-"},"source":["bert_model.save_weights(path+\"/squad_wordpiece.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHmMOi6bhPxm"},"source":["버트 모형을 다시 훈련합니다.  \n","이번에는 validation_split을 입력하지 않아서 전체 데이터가 훈련 되도록 만들어 줍니다."]},{"cell_type":"code","metadata":{"id":"98LafkcmWAc6"},"source":["bert_model.compile(optimizer=RAdam(learning_rate=0.00003, decay=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n","bert_model.fit(train_x, train_y, batch_size=10, shuffle=False, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h0xs7diprDw1"},"source":["bert_model.save_weights(path+\"/squad_wordpiece_2.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"154p7E_ihh2b"},"source":["버트 모형을 한번 더 훈련시켜 줍니다. learning_rate을 0.00001로 바꿔 줍니다."]},{"cell_type":"code","metadata":{"id":"qnZ0bdoXCJVr"},"source":["bert_model.compile(optimizer=RAdam(learning_rate=0.00001, decay=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WkGimQRChdMZ"},"source":["bert_model.save_weights(path+\"/squad_wordpiece_3.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e-HySrkKNV_e"},"source":["재사용을 위해 bert_model을 지드라이브에 저장해줍니다."]},{"cell_type":"markdown","metadata":{"id":"f0ZgfR6MRIul"},"source":["버트 모형을 로드해줍니다. 이미 로드하였던 모델에 계수들만 살짝 얹혀 줍니다."]},{"cell_type":"code","metadata":{"id":"DwMLCYp0TzTv"},"source":["bert_model = get_bert_finetuning_model(model)\n","bert_model.load_weights(path+\"/squad_wordpiece_3.h5\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YeXe8E-0i1si"},"source":["Test data set에 대한 bert_input을 만들어 줍니다.  \n","Train data set과는 다르게 label을 생성하지 않습니다."]},{"cell_type":"code","metadata":{"id":"q_WdnjmMYZ-N"},"source":["def convert_pred_data(question, doc):\n","    global tokenizer\n","    indices, segments = [], []\n","    ids, segment = tokenizer.encode(question, doc, max_len=SEQ_LEN)\n","    indices.append(ids)\n","    segments.append(segment)\n","    indices_x = np.array(indices)\n","    segments = np.array(segments)\n","    return [indices_x, segments]\n","\n","def load_pred_data(question, doc):\n","    data_x = convert_pred_data(question, doc)\n","    return data_x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4LmVp-5lOnR"},"source":["질문과 문장을 받아 답을 알려주는 함수를 정의합니다."]},{"cell_type":"code","metadata":{"id":"Engs2SIlW0SP"},"source":["def predict_letter(question, doc):\n","  \n","  test_input = load_pred_data(question, doc)\n","  test_start, test_end = bert_model.predict(test_input)\n","  \n","  indexes = tokenizer.encode(question, doc, max_len=SEQ_LEN)[0]\n","  start = np.argmax(test_start, axis=1).item()\n","  end = np.argmax(test_end, axis=1).item()\n","  start_tok = indexes[start]\n","  end_tok = indexes[end]\n","  print(\"Question : \", question)\n","  \n","  print(\"-\"*50)\n","  print(\"Context : \", end = \" \")\n","  \n","  def split_text(text, n):\n","    for line in text.splitlines():\n","        while len(line) > n:\n","           x, line = line[:n], line[n:]\n","           yield x\n","        yield line\n","\n","  \n","\n","  for line in split_text(doc, 150):\n","    print(line)\n","\n","  print(\"-\"*50)\n","  print(\"ANSWER : \", end = \" \")\n","  print(\"\\n\")\n","  sentences = []\n","  \n","  for i in range(start, end+1):\n","    token_based_word = reverse_token_dict[indexes[i]]\n","    sentences.append(token_based_word)\n","    print(token_based_word, end= \" \")\n","  \n","  print(\"\\n\")\n","  print(\"Untokenized Answer : \", end = \"\")\n","  for w in sentences:\n","    if w.startswith(\"##\"):\n","      w = w.replace(\"##\", \"\")\n","    else:\n","      w = \" \" + w\n","    \n","    print(w, end=\"\")\n","  print(\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YBRodQAqlXf7"},"source":["SQAUD 데이터 셋에서 test 용도로 쓰이는 dev 파일을 PANDAS DATAFRAME 형식으로 불러오는 함수를 정의합니다.  \n","train 데이터와 모양이 약간 다르기 때문에, 함수를 새로 정의해야 합니다."]},{"cell_type":"code","metadata":{"id":"6FpWAQR1ZqGt"},"source":["def squad_json_to_dataframe_dev(input_file_path, record_path = ['data','paragraphs','qas','answers'],\n","                           verbose = 1):\n","    \"\"\"\n","    input_file_path: path to the squad json file.\n","    record_path: path to deepest level in json file default value is\n","    ['data','paragraphs','qas','answers']\n","    verbose: 0 to suppress it default is 1\n","    \"\"\"\n","    if verbose:\n","        print(\"Reading the json file\")    \n","    file = json.loads(open(input_file_path).read())\n","    if verbose:\n","        print(\"processing...\")\n","    # parsing different level's in the json file\n","    js = pd.io.json.json_normalize(file , record_path )\n","    m = pd.io.json.json_normalize(file, record_path[:-1] )\n","    r = pd.io.json.json_normalize(file,record_path[:-2])\n","    \n","    #combining it into single dataframe\n","    idx = np.repeat(r['context'].values, r.qas.str.len())\n","    m['context'] = idx\n","    main = m[['id','question','context','answers']].set_index('id').reset_index()\n","    main['c_id'] = main['context'].factorize()[0]\n","    if verbose:\n","        print(\"shape of the dataframe is {}\".format(main.shape))\n","        print(\"Done\")\n","    return main"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qMVFr75nZq9G"},"source":["input_file_path ='dev-v1.1.json'\n","record_path = ['data','paragraphs','qas','answers']\n","verbose = 0\n","dev = squad_json_to_dataframe_dev(input_file_path=input_file_path,record_path=record_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1QSzFOnPmv4g"},"source":["TEST DATA가 잘 불려왔는지 확인해 보겠습니다."]},{"cell_type":"code","metadata":{"id":"tluhycV8ZxqF"},"source":["dev"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I9614q2jmcq-"},"source":["테스트 데이터에 대해서 결과를 확인합니다.  \n","훈련에 사용하지 않은 테스트 데이터에 대한 예측을 제법 잘 수행하는 것을 보실 수 있겠습니다."]},{"cell_type":"code","metadata":{"id":"IBmiDNInax0z"},"source":["import random\n","for i in random.sample(range(100),100):\n","  doc = dev['context'][i]\n","  question = dev['question'][i]\n","  answers = dev['answers'][i]\n","  predict_letter(question, doc)\n","  print(\"\")\n","  print(\"real answer : \", answers)\n","  print(\"\")"],"execution_count":null,"outputs":[]}]}